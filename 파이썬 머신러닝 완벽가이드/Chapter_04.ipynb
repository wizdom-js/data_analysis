{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 04. 분류\n",
    "## 01. 분류(Classification)의 개요\n",
    "- 지도학습 : 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝의 방식\n",
    "\n",
    "\n",
    "- 분류(Classification) : 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측\n",
    "    - 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것.\n",
    "    - 지도학습의 대표적인 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류의 머신러닝 알고리즘\n",
    "    - 베이즈 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)\n",
    "    - 독립변수와 종속변수 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)\n",
    "    - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)\n",
    "    - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)\n",
    "    - 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘\n",
    "    - 심층 연결 기반의 신경망(Neural Network)\n",
    "    - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)\n",
    "        - 분류에서 가장 각광을 받는 방법 중 하나. 정형 데이터의 예측 분석 영역에서 매우 높은 예측 성능으로 인해 애용되고 있다.\n",
    "        - 서로 다른(또는 같은) 알고리즘을 단순히 결합한 형태도 있으나, 배깅과 부스팅 방식이 일반적이다.\n",
    "            - 배깅 방식 : 대표적으로 랜덤 포레스트가 있다.(앙상블의 기본 알고리즘으로, 일반적으로 사용함) 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많은 분석가가 애용하는 알고리즘이다.\n",
    "            - 부스팅 방식 : 근래의 앙상블 방법은 부스팅 방식으로 지속해서 발전하고 있다. 그래디언 부스팅의 경우 계속적으로 발전된 알고리즘이 등장하면서 정형 데이터의 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡았다.\n",
    "        - 결정 트리와 앙상블\n",
    "            - 결정 트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘이다. 또한 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적다.\n",
    "                - 하지만 예측 성능을 향상시키기 위해 복잡한 규칙구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점이 있다.(하지만 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용)\n",
    "            - 앙상블은 매우 많은 여러개의 약한 학습기(즉, 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성을을 향상시키는데, 결정트리가 좋은 약한 학습기가 되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.결정트리\n",
    ": 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것. ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘.\n",
    "- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다.\n",
    "- 결정 트리의 구조\n",
    "    - 규칙 노드(Decision Node)로 표시된 노드 -> 규칙 조건\n",
    "    - 리프 노드(Leaf Node)로 표시된 노드 -> 결정된 클래스 값\n",
    "    - 서브 트리(Sub Tree) : 새로운 규칙 조건마다 생성됨\n",
    "- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙노드가 만들어진다. \n",
    "- 하지만, 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 이야기이고, 이는 곧 과적합으로 이어진다. \n",
    "- 즉, 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면\n",
    ": 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 한다.\n",
    "- 이를 위해서는 어떻게 트리를 분할할 것인가가 중요. 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요\n",
    "    - 데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미친다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결정 노드\n",
    ": 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듭니다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. \n",
    "##### 정보의 균일도를 측정하는 대표적인 방법 : 엔트로피를 이용한 정보이득 지수, 지니 계수\n",
    "- 정보이득 : 엔트로피라는 개념을 기반으로 한다. 1-엔트로피 지수. 결정트리는 이 정보 이득 지수로 분할 기준을 정한다. 즉, 정보 이득이 높은 속성을 기준으로 분할\n",
    "    - 엔트로피 : 주어진 데이터 집합의 혼잡도. 서로 다른 값이 섞여있으면 엔트로피가 높다.\n",
    "- 지니 계수  : 불평등 지수를 나타낼 때 사용하는 계수이다. 0이 가장 평등 1로 갈수록 불평등. 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할한다.\n",
    "    - DecisionTreeClassifier는 기본으로 지니계수를 이용해 데이터 세트를 분할한다.\n",
    "    \n",
    "\n",
    "- 결정 트리의 일반적인 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 모델의 특징\n",
    "#### 장점 \n",
    ": 정보의 균일도라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적이다. 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지안핟. \n",
    "#### 단점\n",
    ": 과적합으로 정확도가 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 파라미터\n",
    "-  DecisionTreeClassfier  : 분류를 위한 클래스\n",
    "- DecisionTreeRegressor : 회귀를 위한 클래스\n",
    "- 사이킷런의 결정 트리 구현은 CART(Classification And Regression Trees) 알고리즘 기반이다. 이는 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘이다.\n",
    "\n",
    "#### 파라미터\n",
    "- min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수. 과적합을 제어하는데 사용\n",
    "    - 디폴트 2. 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
    "- min_samples_leaf : 말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수. \n",
    "    - 과적합 제어 용도. \n",
    "    - 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요\n",
    "- max_features : 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행\n",
    "    - int: 대상피처의 개수 / float: 전체 피처 중 대상 피처의 퍼센트 / sqrt: 전체 피처 중 sqrt(전체피처개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처명에서 10개만 추출: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.\n",
    "feature_name_df = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출:', feature_name[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**중복된 피처명을 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_index    42\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,32</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              column_index\n",
       "column_name                               \n",
       "fBodyAcc-bandsEnergy()-1,16              3\n",
       "fBodyAcc-bandsEnergy()-1,24              3\n",
       "fBodyAcc-bandsEnergy()-1,8               3\n",
       "fBodyAcc-bandsEnergy()-17,24             3\n",
       "fBodyAcc-bandsEnergy()-17,32             3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dup_df = feature_name_df.groupby('column_name').count()\n",
    "print(feature_dup_df[feature_dup_df['column_index'] > 1].count())\n",
    "feature_dup_df[feature_dup_df['column_index'] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                  columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                  columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] =  new_feature_name_df['column_name'].astype('U32')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_human_dataset():\n",
    "    \n",
    "    feature_name_df = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt', sep='/s+', \n",
    "                                 header = None, names = ['column_index', 'column_name'])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 로딩. 칼럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/X_train.txt', sep='/s+', names = feature_name)\n",
    "    X_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/X_test.txt', sep='/s+', names = feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로딩하고 칼럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/y_train.txt', sep='/s+', header = None, names = ['action'])\n",
    "    y_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/y_test.txt', sep='/s+', header = None, names = ['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. GBM(Gradient Boosting Machine)\n",
    "### GBM의 개요 및 실습 \n",
    "#### 부스팅의 대표적인 구현\n",
    "- AdaBoost(Adaptive Boosting)\n",
    "    - 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘\n",
    "    - 즉, 약한 학습기가 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측을 수행한다.\n",
    "    - 예를 들어 첫번째 학습기에 가중치 0.3 두번째에는 0.5 세번째에는 0.8을 부여한 후 모두 결합해 예측을 수행한다.\n",
    "    \n",
    "    \n",
    "- GBM(Gradient Boost Machine) \n",
    "    - 에이다부스트와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이이다.\n",
    "        - 오류값 : 실제값 - 예측값\n",
    "        - 경사 하강법 : 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것\n",
    "    - 분류는 물론, 회귀도 가능\n",
    "    - GradientBoostingClassifier 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-cd842b496d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0m\u001b[1;32m    410\u001b[0m                                    dtype=DTYPE, multi_output=True)\n\u001b[1;32m    411\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'"
     ]
    }
   ],
   "source": [
    "# 사용자 행동 데이터 세트 예측 분류\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행시간 측정을 취함. 시작 시간 설정\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간: {0:.1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로, GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다.\n",
    "\n",
    "그러나 수행시간이 오래 걸리고(GBM이 극복해야 할 중요한 과제), 하이퍼 파라미터 튜닝 노력도 더 필요하다.\n",
    "(반면에 랜덤 포레스트는 상대적으로 빠른 수행 시간을 보장해주기 떄문에 더 쉽게 예측 결과 도출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM 하이퍼 파라미터 튜닝\n",
    "- loss : 경사 하강법에서 사용할 비용함수 지정(default : deviance)\n",
    "\n",
    "\n",
    "- learning rage : GBMdㅣ 학습을 진행할 때마다 적용하는 학습률. weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수.\n",
    "    - 0~1 사이 값을 지정할 수 있으며, 기본값은 0.1이다.\n",
    "    - 너무 작은 값 적용할 시 : \n",
    "        - 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성 높다.\n",
    "        - 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다.\n",
    "    - 많은 weak learner :\n",
    "        - 순차적인 반복이 필요해 수행시간이 오래걸린다.\n",
    "        - 최소 오류 값을 찾지 못하고 그냥 지나쳐버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행 가능\n",
    "    - 따라서 이러한 특성때문에 n_estimator와 상호 보완적으로 조합해 사용한다.\n",
    "        - learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측성능이 조금씩 좋아질 수 있으나 시간이 오래걸리고 성능 역시 현격히 좋아지지는 않음\n",
    "\n",
    "\n",
    "- n_estimators : weak learner의 개수\n",
    "    - weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있지만, 개수가 많을 수록 시간 오래걸림\n",
    "    \n",
    "\n",
    "- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율\n",
    "    - 기본값 1, 이는 전체 학습 데이터를 기반으로 학습한다는 의미 \n",
    "    - 과적합이 염려되는 경우 1보다 작은 값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV를 이용해 하이퍼 파라미터를 최적화\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 500],\n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_cv = GridSearchCV(gb_clf, parma_gird=params, cv=2, verbose=1)\n",
    "gird_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 설정 그대로 테스트 데이터 세트에 적용해 에측 정확도 확인\n",
    "\n",
    "# GridSearchCV를 이용해 최적으로 학습된 estimator로 예측 수행\n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. XGBoost(eXtra Gradient Boost)\n",
    ": 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나\n",
    "- GBM에 기반하고 있지만, GBM의 단점인 느린 수행시간 및 과적합 규제 부재 등의 문제를 해결했다.\n",
    "    - 병렬 CPU 환경에서 병렬학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다.\n",
    "    \n",
    "    \n",
    "- **XGBoost 장점**\n",
    "    - 뛰어난 예측 성능 발휘\n",
    "    - GBM 대비 빠른 수행 시간\n",
    "        - XBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 가지고 있다. \n",
    "        - GBM에 비해 수행시간이 빠르다는 것이지 다른 머신러닝 알고리즘에 비해 빠르다는 의미는 아님.\n",
    "    - 과적합 규제\n",
    "        - XGBoost는 자체에 과적합 규제 기능으로 좀 더 강한 내구성을 가진다.\n",
    "    - Tree pruning(나무 가지치기)\n",
    "        - GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조정하기도 하지만, tree pruning으로 더이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.\n",
    "    - 자체 내장된 교차 검증\n",
    "       - 반복 수행시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차검증을 수행해 최적화된 바복 수행 횟수를 가질 수 있다.\n",
    "       - 지정된 반복 횟수가 아닌 교차검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단기능이 있다.\n",
    "           - 조기 중단기능은 수행 속도를 향상시킨다.\n",
    "           - n_estimators에 지정한 부스팅 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선한다.\n",
    "    - 결손값 자체 처리 기능\n",
    "    - 자체적으로 성능 평가, 피처 중요도 등의 시각화 기능을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost의 파이썬 패키지명 : xgboost\n",
    "    - 초기 출시에는 사이킷런과 호환되지 않는 독자적인 XgBoost 전용의 패키지였다.\n",
    "    - 사람들이 사이킷런을 많이 사용하기 때문데 사이킷런과 연동할 수 있는 래퍼 클래스(Wrapper class)를 제공했다.\n",
    "- XGBoost 패키지의 사이킷런 래퍼 클래스 : XGBClassifier, XGBRegressor\n",
    "    - 이를 이용하면 사이킷런 estimator가 학습을 위해 사용하는 fit과 predict과 같은 표준 사이킷런 개발 프로세스 및 다양한 유틸리티 활용 가능\n",
    "- 사이킷런 래퍼 XGBoost 모듈은 사이킷런의 다른 estimator와 사용법이 같은데, 파이썬 네이티브 XGBoost는 고유의 API 하이퍼 파라미터를 이용한다. 크게 다르지는 않으나 몇 가지 주의할 점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 하이퍼 파라미터\n",
    "- 파이썬 래퍼 XGBoost 모듈과 사이킷런 래퍼 XGBoost 모듈의 일부 하이퍼 파라미터는 약간 다르므로 주의가 필요하다.\n",
    "    - 동일한 기능을 의미하는 하이퍼 파라미터지만, 사이킷런 파라미터의 범용화된 이름 규칙에 따라 파라미터 명이 달라짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 일반 파라미터 >\n",
    ": 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터.(디폴트 파라미터 값을 바꾸는 경우는 거의 없다.)\n",
    "- **booster**\n",
    "    - gbtree(tree based model)또는 gblinear(linear model) 선택. \n",
    "        - 디폴트는 gbree\n",
    "- **silent**\n",
    "    - 디폴트는 0, 출력 메시지를 나타내고 싶지 않을 경우 1\n",
    "- **nthread** \n",
    "    - CPU의 실행 스레드 개수 조정\n",
    "        - 디폴트는 CPU의 전체 스레드를 다 사용하는 것.\n",
    "    - 멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해서 ML 애플리케이션을 구동하는 경우에 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 부스터 파라미터 >\n",
    ": 트리 최적화, 부스팅, regularization 등과 관련 파라미터 등을 지칭한다.\n",
    "- **eta[default=0.3, alias:learning_rate]**\n",
    "    - GBM의 학습률(learning_rate)와 같은 파라미터.\n",
    "    - 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값.\n",
    "        - 사이킷런 래퍼 클래스를 이용할경우 eta는 learning_rate 파라미터로 대체되며, 디폴트는 0.1이다. 보통은 0.01 ~ 0.2 사이의 값을 선호한다.\n",
    "        \n",
    "        \n",
    "- **num_boost_rounds**\n",
    "    - GBM의 n_estimator과 같은 파라미터이다.\n",
    "    \n",
    "    \n",
    "- **min_child_weight[default=1]**\n",
    "    - 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합\n",
    "    - 클수록 분할을 자제한다.\n",
    "    - 과적합을 조절하기 위해 사용\n",
    "    \n",
    "    \n",
    "- **gamma[default=0, alias:min_split_loss]**\n",
    "    - 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값.\n",
    "    - 새당 값보다 큰 손실(loss)이 감소된 경우에 리프 노드를 분리한다.\n",
    "    - 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **max_depth[default=6]**\n",
    "    - 트리 기반 알고리즘의 max_depth와 같다.\n",
    "    - 0을 지정하면 깊이 제한 X. 보통은 3~10 사이의 값을 적용한다.\n",
    "    - depth가 높으면 과적합 가능성이 높아진다.\n",
    "    \n",
    "    \n",
    "- **sub_sample[default=1]**\n",
    "    - GBM의 subsample과 동일. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정ㅎ나다.\n",
    "    - 0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는데 사용한다. \n",
    "    - 일반적으로 0.5 ~ 1 사이의 값을 사용한다.\n",
    "    \n",
    "    \n",
    "- **colsample_bytree[default=1]**\n",
    "    - GBM의 max_features와 유사. 트리 생성에 필요한 피처를 임의로 샘플링하는데 사용\n",
    "    - 매우 많은 피처가 있는 경우 과적합을 조정하는데 적용\n",
    "    \n",
    "    \n",
    "- **lambda[default=1, alias:reg_lambda]**\n",
    "    - L2 Regularization 적용 값. 피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **alpha[default=0, alias:reg_alpha]**\n",
    "    - L1 Regularization 적용 값.피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **scale_pos_wight[default=1]**\n",
    "    - 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 학습 태스크 파라미터 >\n",
    "- **objective**\n",
    "    - 최솟값을 가져야 할 손실 수 정의. \n",
    "    - 주로 사용되는 손실 함수는 이진 분류인지 다중 분류인지에 따라 달라진다.\n",
    "    \n",
    "    \n",
    "- **binary:logistic**\n",
    "    - 이진 분류일 때 적용\n",
    "    \n",
    "- **multi:softmax**\n",
    "    - 다중 분류일 때 적용.\n",
    "    - 손실함수가 multi:softmax일 경우에는 레이블 클래스 개수인 num_class 파라미터를 지정해야 한다.\n",
    "\n",
    "\n",
    "- **multi:softpob**\n",
    "    - multi:softmax와 유사하나, 개별 레이블 클래스의 해당되는 예측 확률을 반환한다.\n",
    "\n",
    "\n",
    "- **eval_metric**\n",
    "    - 검증에 사용되는 함수를 정의.\n",
    "    - 기본값은 회귀인 경우는 rmse, 분류일 경우에는 error이다.\n",
    "    - **eval_metirc의 값 유형**\n",
    "        - **rmse** : 평균 제곱근 오차(Root Mean Square Error)\n",
    "            - 잔차의 제곱합을 산술평균한 값의 제곱근. 관측값들간의 상호간 편차\n",
    "        - **mae** : 평균 절대 오차(Mean Absolute Error)\n",
    "            - 오차의 절대값의 평균\n",
    "        - **logloss** : 음의 로그 가능도(Negative log-likelihood)\n",
    "            - 로그가능도 x -1. 확률 변수가 독립확률변수로 나누어지는 경우와 같이 확률분포함수가 곱셈꼴로 나올때 미분계산의 편의성을 위해 사용한다.\n",
    "        - **error** : 이진분류 오류율(Binary classification error rate(0.5 threshold))\n",
    "        - **merror** : 다중 클래스 분류 오류율(Multicalss classification error rate)\n",
    "        - **mlogloss** : 다중클래스의 로그 손실(Multiclass logloss)\n",
    "        - **acu** : 곡선 하위 영역(Area under the curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뛰어난 알고리즘일수록 파라미터를 튜닝할 필요가 적다. 또한 파라미터 튜닝에 들이는 공수 대비 성능 향상 효과가 높지 않은 경우가 대부분이다.\n",
    "\n",
    "파라미터를 튜닝하는 경우의 수는 여러 가지 상황에 따라 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과적합 문제가 심각할 경우 다음과 같이 적용해본다.\n",
    "    - eta값을 낮춘다.(0.01 ~ 0.1) eta값을 낮출 경ㅇ우 num_round(또는 n_estimators)는 반대로 높여줘야함\n",
    "    - max_depth 값을 낮춘다.\n",
    "    - min_child_weight 값을 높인다.\n",
    "    - gamma 값을 높인다.\n",
    "    - subsample과 colsample_bytree를 조정하는 것도 트리가 너무 복잡하게 생성되는 것을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 버전 확인\n",
    "import xgboost \n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 적용 - 위스콘신 유방암 예측\n",
    "위스콘신 유방암 데이터 세트를 활용하여 파이썬 래퍼 XGBoost API의 사용법을 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 및 데이터 로드\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance # 피처의 중요도 시각화\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wanings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_features = dataset.data\n",
    "y_label = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\n",
    "cancer_df['target'] = y_label\n",
    "cancer_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "종양의 크기와 모양에 관련된 많은 속성이 숫자형 값으로 되어 있다.\n",
    "\n",
    "타깃 레이블 값의 종류는 악성인 malignant가 0, 양성인 benign이 1값으로 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 값의 분포 확인\n",
    "\n",
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "print(x_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬 래퍼 XGBoost와 사이킷런의 차이\n",
    "먼저 눈에 띄는 차이는 파이썬 래퍼 XGBoost는 학습용과 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다.\n",
    "- DMatrix : 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트\n",
    "    - 주요 파라미터\n",
    "        - data : 피처 데이터 세트\n",
    "        - label : 분류의 경우에는 레이블 데이터세트, 회귀의 경우는 숫자형인 종솟값 데이터 세트\n",
    "    - DMatrix는 넘파이 이외에 libsvm txt 포맷 파일, xgboost 이진 버퍼 파일을 파라미터로 입력받아 변환할 수 있다. \n",
    "        - 판다스의 데이터프레임으로 데이터 인터페이스를 하기 위해서는 DataFrame.values를 이용해 넘파이로 일차 변환한 뒤에 이를 이용해 DMatrix 변환을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 형태의 학습 데이터 세트와 테스트 데이터 세트를 DMatrix로 변환\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.Dmatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 래퍼 XGBoost 모듈인 xgboost를 이용해 학습을 수행하기 전에, 먼저 XGBoost의 하이퍼 파라미터 설정. 주로 !!딕셔너리 형태!!로 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'max_depth':3,\n",
    "          'eta':0.1,\n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'earl_stoppings':100 # 조기 중단할 수 있는 최소 반복 횟수\n",
    "}\n",
    "num_rounds = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 하이퍼 파라미터를 xgboost 모듈의 train()함수에 파라미터로 전달한다.(사이킷런의 경우는 Estimator의 생성자를 하이퍼 파라미터로 전달)\n",
    "\n",
    "\n",
    "- **early_stopping_rounds 파라미터 입력**을 통해 **조기중단**할 수 있도록 설정한다.\n",
    "    - early_stopping_rounds 파라미터를 설정해 조기 중단을 수행하기 위해서는 반드시 **eval_set**과 **eval_metric**이 함께 설정되어야 한다.\n",
    "        - XGBoost는 반복마다 eval_set으로 지정된 데이터 세트에서 eval_metric의 지정된 평가 지표로 예측 오류를 측정한다.\n",
    "        - **eval_set** : 성능 평가를 수행할 평가용 데이터 세트 설정\n",
    "        - **eval_metric** : 평가 세트에 적용할 성능 평가 방법. 분류일 경우 주로 error, logloss를 적용\n",
    "    - xgboost train()의 **evals 파라미터**에 학습된 데이터 세트와 eval 데이터 세트를 명기해주면, 평가를 eval 데이터 세트에 수행하면서 조기 중단을 적용할 수 있다. 조기 중단을 수행하려면 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명기\n",
    "wlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train()함수의 파라미터로 전달\n",
    "# 반복시마다 evals에 표시된 데이터 세트에 대해 평가지표 결과 출력된다.\n",
    "# train()은 학습이 완료된 모델 객체를 반환\n",
    "xgb_model = xgb.train(params = paras, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=100, evals=wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 train()함수를 호출해 학습이 완료된 모델 객체를 반환하게 되는데, 이 모델 객체는 예측을 위해 predict() 메서드를 이용한다.\n",
    "    - 유의할 점은 사이킷런의 predict()메서드는 예측 결과 클래스 값(즉 0, 1)을 반환하는데 반해 xgboost의 predict()는 예측 결괏값이 아닌 예측 결과를 추정할 수 있는 확률 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict() 수행 결괏값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정해 리스트 객체인 preds에 저장\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시: ', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델의 예측 성능 평가\n",
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xboost의 plot_importance() : 피처의 중요도를 막대그래프 형식으로 나타냄\n",
    "    - 기본 평가 지표로 f1스코어를 기반으로 한다.\n",
    "    - 호출 시 파라미터로 앞에서 학습이 완료된 앞델 객체 및 맷플롯립의 ax 객체를 입력하기만 하면 된다.\n",
    "    - (사이킷런은 estimator 객체의 feature_importances_속성을 이용해 직접 시각화 코드를 작성해야 한다.)\n",
    "    - 유의 할 점\n",
    "        - xgboost 넘파이 기반의 피처 데이터로 학습 시에 피처명을 제대로 알 수가 없으므로 f0, f1와 같이 피처 순서별로 f자 뒤에 숫자를 붙여서 X축에 피처들로 나열한다. (f0이 첫번째 피처)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(xgb_model, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost 모듈의 to_graphviz() API : 이를 이용하면 주피터 노트북에 바로 규칙 트리 구조를 그릴 수 있다.\n",
    "    - Graphviz 프로그램과 패키지가 설치돼 있어야 한다.\n",
    "    - xgboost.to_graphviz()내에 파라미터로 학습이 완료된 모델 객체와 Graphviz가 참조할 파일명을 입력해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv( ) : 파이썬 래퍼 XGBoost에서 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 제공\n",
    "    - **파라미터**\n",
    "        - **params** (dict) : 부스터 파라미터\n",
    "        - **dtrain** (DMatrix) : 학습 데이터\n",
    "        - **num_boost_round** (int) : 부스팅 반복 횟수\n",
    "        - **nfold** (int) : CV 폴드 개수\n",
    "        - **stratified** (bool) : CV 수행 시 층화 표본 추출(stratified sampling) 수행 여부\n",
    "        - **metrics** (string or list of string) : CV 수행 시 모니터링할 성능 평가 지표\n",
    "        - **early_stopping_rounds** (int) : 조기 중단을 활성화시킴. 반복 횟수 지정\n",
    "    \n",
    "    \n",
    "    - 데이터프레임 형태로 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 래퍼 XGBoost의 개요 및 적용\n",
    "- 다른 estimator와 동일하게 fit과 predict만으로 학습과 예측이 가능하고, GridSearchCV, Pipeline등 사이킷런의 다른 유틸리티를 그대로 사용할 수 있다.\n",
    "    - 따라서 기존의 다른 머신러닝 알고리즘으로 만들어놓은 프로그램이 있더라도 알고리즘 클래스만 XGBoost 래퍼 클래스로 바꾸면 기존 클래스 그대로 사용 가능\n",
    "    \n",
    "    \n",
    "- XGBClassifier(분류를 위한 래퍼 클래스), XGBRegressor(회귀를 위한 래퍼 클래스)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBClassifier는 기존 사이킷런에서 일반적으로 사용하는 하이퍼 파라미터와 호환성을 유지하기 위해 파이썬 래퍼 xgboost 모듈에서 사용하던 네이티브 하이퍼 파라미터 몇 개를 다음과 같이 변경했다.\n",
    "    - eta -> learning_rate\n",
    "    - sub_sample -> subsample\n",
    "    - lambda -> reg_lambda\n",
    "    - alpha -> reg_alpha\n",
    "    \n",
    "\n",
    "- xgboost의 n_estimator와 num_boost_round 하이퍼 파라미터는 서로 동일\n",
    "    - 만일 두개가 동시에 사용 되면\n",
    "        - 파이썬 래퍼 XGBoost는 num_boost_round 파라미터 적용\n",
    "        - XGBClassifier와 같은 사이킷런 래퍼 클래스에서는 n_estimator 파라미터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위스콘신 대학병원의 유방암 데이터 세트를 분류를 위한 래퍼 클래스인 XGBClassifier를 이용해 예측\n",
    "\n",
    "# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb_wrapper.fit(X_train, y_trian)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 성능 평가\n",
    "\n",
    "get_clf_eval(y_test, w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런 래퍼 XGBoost에서의 조기 중단 수행\n",
    "    - 조기 중단 관련한 파라미터를 fit()에 입력한다.\n",
    "        - early_stopping_rounds : 평가 지표가 향상될 수 있는 반복 횟수 정의\n",
    "        - eval_metic : 조기 중단을 위한 평가 지표\n",
    "        - eval_set : 성능 평가를 수행할 데이터 세트\n",
    "            - (학습데이터가 아니라 별도의 데이터 세트여야한다. 학습시에는 완전히 알려지지 않은 데이터 세트 사용해야함. 왜냐면 학습 시에 미리 참고가 되어 과적합할 수 있기 때문)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 예제에서는 데이터 세트의 크기가 작아 테스트 데이터를 평가용으로 사용했다. 이부분을 주지\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_esimators=400, learning_rate=0.1, max_depth=3)\n",
    "evals = [(X_test, y_test)]\n",
    "xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "# verbose=True : 부팅시 상세정보를 보여줌으로써, 어느 부분에서 어떤 지연이 있는지 확인 가능\n",
    "ws100_preds = xgb_wrapper.predict(X_test)\n",
    "ws100_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "211번 반복시 logloss가 0.085593이고 311번 반복 시 logloss가 0.085948인데, 211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에 더이상 반복하지 않고 멈추었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 중단으로 학습된 XGBClassifier의 예측성능\n",
    "get_clf_eval(y_test, ws100_preds, ws100_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기 중단을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. \n",
    "\n",
    "아직 성능이 향상될 여지가 있음에도 불구하고 n번 반복하는 동안 성능 평가 지표가 향상되지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측성능이 나빠질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_rounds를 10으로 설정하고 재학습\n",
    "xgb_wrapper.fit(X_train, y_train, earl_stopping_rounds=10, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "ws10_preds = xgb_wrapper.predict(X_test)\n",
    "ws10_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, ws10_preds, ws10_preds_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early_stopping_rounds=100일때보다 정확도가 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 피처의 중요도를 시각화하는 모듈인 plot_importance()에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 도출해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "# 사이킷런 wrapper 클래스를 입력해도 무방\n",
    "plot_importance(xgb_wrapper, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
