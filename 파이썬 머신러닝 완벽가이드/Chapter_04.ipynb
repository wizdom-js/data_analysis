{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 04. 분류\n",
    "## 01. 분류(Classification)의 개요\n",
    "- 지도학습 : 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝의 방식\n",
    "\n",
    "\n",
    "- 분류(Classification) : 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측\n",
    "    - 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것.\n",
    "    - 지도학습의 대표적인 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류의 머신러닝 알고리즘\n",
    "    - 베이즈 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)\n",
    "    - 독립변수와 종속변수 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)\n",
    "    - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)\n",
    "    - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)\n",
    "    - 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘\n",
    "    - 심층 연결 기반의 신경망(Neural Network)\n",
    "    - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)\n",
    "        - 분류에서 가장 각광을 받는 방법 중 하나. 정형 데이터의 예측 분석 영역에서 매우 높은 예측 성능으로 인해 애용되고 있다.\n",
    "        - 서로 다른(또는 같은) 알고리즘을 단순히 결합한 형태도 있으나, 배깅과 부스팅 방식이 일반적이다.\n",
    "            - **배깅 방식** : 대표적으로 랜덤 포레스트가 있다.(앙상블의 기본 알고리즘으로, 일반적으로 사용함) 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많은 분석가가 애용하는 알고리즘이다.\n",
    "            - **부스팅 방식** : 근래의 앙상블 방법은 부스팅 방식으로 지속해서 발전하고 있다. 그래디언 부스팅의 경우 계속적으로 발전된 알고리즘이 등장하면서 정형 데이터의 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡았다.\n",
    "        - **결정 트리와 앙상블**\n",
    "            - 결정 트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘이다. 또한 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적다.\n",
    "                - 하지만 예측 성능을 향상시키기 위해 복잡한 규칙구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점이 있다.(하지만 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용)\n",
    "            - 앙상블은 매우 많은 여러개의 약한 학습기(즉, 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성을을 향상시키는데, 결정트리가 좋은 약한 학습기가 되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 결정트리\n",
    ": 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것. ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘.\n",
    "- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다.\n",
    "- 결정 트리의 구조\n",
    "    - 규칙 노드(Decision Node)로 표시된 노드 -> 규칙 조건\n",
    "    - 리프 노드(Leaf Node)로 표시된 노드 -> 결정된 클래스 값\n",
    "    - 서브 트리(Sub Tree) : 새로운 규칙 조건마다 생성됨\n",
    "- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙노드가 만들어진다. \n",
    "- 하지만, 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 이야기이고, 이는 곧 과적합으로 이어진다. \n",
    "- 즉, 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면\n",
    ": 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 한다.\n",
    "- 이를 위해서는 어떻게 트리를 분할할 것인가가 중요. **최대한 균일한 데이터 세트를 구성**할 수 있도록 분할하는 것이 필요\n",
    "    - 데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미친다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결정 노드\n",
    ": 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듭니다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. \n",
    "##### 정보의 균일도를 측정하는 대표적인 방법 : 엔트로피를 이용한 정보이득 지수, 지니 계수\n",
    "- **정보이득** : 엔트로피라는 개념을 기반으로 한다. 1-엔트로피 지수. 결정트리는 이 정보 이득 지수로 분할 기준을 정한다. 즉, 정보 이득이 높은 속성을 기준으로 분할\n",
    "    - 엔트로피 : 주어진 데이터 집합의 혼잡도. 서로 다른 값이 섞여있으면 엔트로피가 높다.\n",
    "- **지니 계수**  : 불평등 지수를 나타낼 때 사용하는 계수이다. 0이 가장 평등 1로 갈수록 불평등. 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할한다.\n",
    "    - DecisionTreeClassifier는 기본으로 지니계수를 이용해 데이터 세트를 분할한다.\n",
    "    \n",
    "\n",
    "- 결정 트리의 일반적인 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 모델의 특징\n",
    "#### 장점 \n",
    ": 정보의 균일도라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적이다. 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지않음.\n",
    "#### 단점\n",
    ": 과적합으로 정확도가 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 파라미터\n",
    "-  DecisionTreeClassfier  : 분류를 위한 클래스\n",
    "- DecisionTreeRegressor : 회귀를 위한 클래스\n",
    "- 사이킷런의 결정 트리 구현은 CART(Classification And Regression Trees) 알고리즘 기반이다. 이는 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘이다.\n",
    "\n",
    "#### 파라미터\n",
    "- **min_samples_split** : 노드를 분할하기 위한 최소한의 샘플 데이터 수. 과적합을 제어하는데 사용\n",
    "    - 디폴트 2. 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
    "    \n",
    "    \n",
    "- **min_samples_leaf** : 말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수. \n",
    "    - 과적합 제어 용도. \n",
    "    - 값을 키우면 더 이상 분할하지 않고, 리프 노드가 될 수 있는 조건이 완화된다.\n",
    "    - 즉, min_samples_leaf <= 지정 값 기준만 만족하면 리프노드가 될 수 있다(지니 계수 값이 크더라도)\n",
    "    - 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요\n",
    "    \n",
    "    \n",
    "- **max_features** : 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행\n",
    "    - int: 대상피처의 개수 / float: 전체 피처 중 대상 피처의 퍼센트 / sqrt: 전체 피처 중 sqrt(전체피처개수) / auto: sqrt와 동일 / log: 전체 피처 중 log2(전체피처개수) 선정 \n",
    "    \n",
    "    \n",
    "- **max_depth** : 트리의 최대 깊이를 규정\n",
    "    - 디폴트는 None(-1). 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 떄까지 계속 깊이를 증가시킴\n",
    "    - 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합 할 수 있으므로 적절한 값으로 제어 필요\n",
    "    \n",
    "    \n",
    "- **max_leaf_nodes** : 말단 노드(Leaf)의 최대 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결정트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건이다. 중요한 몇 개의 피처가 명확한 규칙 트리를 만드는 데 크게 기여하며, 모델을 좀 더 간결하고 이상치(Outlier)에 강한 모델을 만들 수 있기 때문이다.\n",
    "\n",
    "- 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 피처의 중요한 역할 지표를 DecisionTreeClassifier 객체의 **feature_importances_** 속성으로 제공한다.\n",
    "    - **feature_importances_**는 ndarray 형태로 값을 반환하며 피처 순서대로 값이 할당 된다. 값이 높을수록 해당 피처의 중요도가 높다는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 모델의 시각화\n",
    "- **Graphviz 패키지** : 결정트리 알고리즘이 어떠한 규칙을 가지고 트리를 생성하는지 시각적으로 보여줄 수 있는 방법. 그래프 기반의 dot파일로 기술된 다양한 이미지를 쉽게 시각화 할 수 있는 패키지.\n",
    "    - 사이킷런의 export_graphviz()에 함수 인자로 학습이 완료된 estimator, 피처의 이름 리스트, 레이블 이름 리스트를 입력하면 학습된 결정 트리 규칙을 실제 트리 형태로 시각화해 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=156)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DecisionTree Calssifier 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\n",
    "                                                   test_size=0.2, random_state=11)\n",
    "\n",
    "# DecisiontreeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_graphviz() 함수 불러오기 \n",
    "# graphviz가 읽어 들여서 그래프 형태로 시각화할 수 있는 출력 파일 생성\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names,\n",
    "               feature_names = iris_data.feature_names, impurity=True ,filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- impurity : 불순도 측정(gini), filled : 색칠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.46.0 (20210118.1747)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"744pt\" height=\"671pt\"\n",
       " viewBox=\"0.00 0.00 744.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 740,-667 740,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#fffdfd\" stroke=\"black\" points=\"268.5,-663 110.5,-663 110.5,-580 268.5,-580 268.5,-663\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-647.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 2.45</text>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-632.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-617.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 120</text>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-602.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [41, 40, 39]</text>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-587.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#e58139\" stroke=\"black\" points=\"170,-536.5 57,-536.5 57,-468.5 170,-468.5 170,-536.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113.5\" y=\"-521.3\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"113.5\" y=\"-506.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 41</text>\n",
       "<text text-anchor=\"middle\" x=\"113.5\" y=\"-491.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [41, 0, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"113.5\" y=\"-476.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.13,-579.91C155.82,-568.65 147.87,-556.42 140.53,-545.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.43,-543.15 135.04,-536.67 137.56,-546.96 143.43,-543.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.85\" y=\"-557.42\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#fafefc\" stroke=\"black\" points=\"343,-544 188,-544 188,-461 343,-461 343,-544\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.5\" y=\"-528.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.55</text>\n",
       "<text text-anchor=\"middle\" x=\"265.5\" y=\"-513.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"265.5\" y=\"-498.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 79</text>\n",
       "<text text-anchor=\"middle\" x=\"265.5\" y=\"-483.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 40, 39]</text>\n",
       "<text text-anchor=\"middle\" x=\"265.5\" y=\"-468.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215.87,-579.91C221.59,-571.1 227.69,-561.7 233.6,-552.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.67,-554.31 239.18,-544.02 230.8,-550.5 236.67,-554.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.37\" y=\"-564.78\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#3ee684\" stroke=\"black\" points=\"257.5,-425 99.5,-425 99.5,-342 257.5,-342 257.5,-425\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-409.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 5.25</text>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-394.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.051</text>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 38</text>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 37, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"178.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M235.32,-460.91C228.64,-451.92 221.49,-442.32 214.6,-433.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.41,-430.96 208.63,-425.02 211.79,-435.13 217.41,-430.96\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#8b49e7\" stroke=\"black\" points=\"431,-425 276,-425 276,-342 431,-342 431,-425\"/>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-409.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.75</text>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-394.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.136</text>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 41</text>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 3, 38]</text>\n",
       "<text text-anchor=\"middle\" x=\"353.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M296.03,-460.91C302.79,-451.92 310.02,-442.32 316.98,-433.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"319.81,-435.12 323.02,-425.02 314.21,-430.91 319.81,-435.12\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#39e581\" stroke=\"black\" points=\"113,-298.5 0,-298.5 0,-230.5 113,-230.5 113,-298.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-283.3\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-268.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 37</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-253.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 37, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-238.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.17,-341.91C123.98,-330.21 110.68,-317.46 98.5,-305.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.72,-303.06 91.08,-298.67 95.88,-308.11 100.72,-303.06\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#8139e5\" stroke=\"black\" points=\"238,-298.5 131,-298.5 131,-230.5 238,-230.5 238,-298.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-283.3\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-268.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-253.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-238.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.58,-341.91C181.13,-331.2 181.72,-319.62 182.28,-308.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185.78,-308.83 182.8,-298.67 178.79,-308.47 185.78,-308.83\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"black\" points=\"427.5,-306 267.5,-306 267.5,-223 427.5,-223 427.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\">sepal length (cm) &lt;= 5.45</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 2, 2]</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.42,-341.91C350.99,-333.56 350.53,-324.67 350.09,-316.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"353.59,-315.83 349.58,-306.02 346.59,-316.19 353.59,-315.83\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#843ee6\" stroke=\"black\" points=\"603.5,-306 445.5,-306 445.5,-223 603.5,-223 603.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 4.85</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.053</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 37</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 1, 36]</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M412.83,-341.91C427.02,-332.2 442.28,-321.76 456.81,-311.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"459,-314.56 465.28,-306.02 455.05,-308.78 459,-314.56\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#8139e5\" stroke=\"black\" points=\"250,-179.5 143,-179.5 143,-111.5 250,-111.5 250,-179.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"196.5\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M295.11,-222.91C279.73,-210.99 262.93,-197.98 247.63,-186.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.35,-183.03 239.31,-179.67 245.07,-188.56 249.35,-183.03\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#9cf2c0\" stroke=\"black\" points=\"426.5,-187 268.5,-187 268.5,-104 426.5,-104 426.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\">petal length (cm) &lt;= 5.45</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.5,-222.91C347.5,-214.65 347.5,-205.86 347.5,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"351,-197.02 347.5,-187.02 344,-197.02 351,-197.02\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#39e581\" stroke=\"black\" points=\"288,-68 175,-68 175,0 288,0 288,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"231.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"231.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"231.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"231.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M304.31,-103.73C294.45,-94.42 283.98,-84.54 274.15,-75.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.45,-72.62 266.78,-68.3 271.65,-77.71 276.45,-72.62\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#8139e5\" stroke=\"black\" points=\"413,-68 306,-68 306,0 413,0 413,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"359.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"359.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"359.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"359.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.97,-103.73C352.88,-95.43 353.84,-86.67 354.76,-78.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.24,-78.62 355.85,-68.3 351.28,-77.86 358.24,-78.62\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c09cf2\" stroke=\"black\" points=\"604.5,-187 444.5,-187 444.5,-104 604.5,-104 604.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\">sepal length (cm) &lt;= 5.95</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"middle\" x=\"524.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.5,-222.91C524.5,-214.65 524.5,-205.86 524.5,-197.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"528,-197.02 524.5,-187.02 521,-197.02 528,-197.02\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#8139e5\" stroke=\"black\" points=\"736,-179.5 623,-179.5 623,-111.5 736,-111.5 736,-179.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.5\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"679.5\" y=\"-149.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 34</text>\n",
       "<text text-anchor=\"middle\" x=\"679.5\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 0, 34]</text>\n",
       "<text text-anchor=\"middle\" x=\"679.5\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>12&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M578.28,-222.91C594.21,-210.88 611.63,-197.73 627.45,-185.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"629.69,-188.49 635.56,-179.67 625.47,-182.9 629.69,-188.49\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#39e581\" stroke=\"black\" points=\"568,-68 455,-68 455,0 568,0 568,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"511.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"511.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"511.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"511.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M519.66,-103.73C518.67,-95.43 517.63,-86.67 516.64,-78.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"520.11,-77.82 515.45,-68.3 513.16,-78.64 520.11,-77.82\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"#8139e5\" stroke=\"black\" points=\"693,-68 586,-68 586,0 693,0 693,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M567.32,-103.73C577.09,-94.42 587.47,-84.54 597.22,-75.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"599.7,-77.73 604.53,-68.3 594.87,-72.66 599.7,-77.73\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fa18f253fa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북상에서 시각화\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 규칙에 따라 트리의 브랜치 노드와 말단 리프 노드가 어떻게 구성되는지 한 눈에 알 수 있게 시각화 되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리프노드 : 더 이상 자식 노드가 없는 노드. 최종 클래스(레이블)값이 결정되는 노드\n",
    "    - 리프노드가 되려면 오직 ㅎㅏ나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 된다.\n",
    "- 브랜치 노드 : 자식 노드가 있는 노드. 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림의 노드 내에 기술된 지표의 의미\n",
    "- pental length(cm) <= 2.45와 같이 피처의 조건이 있는 것은 자식 노드를 만들기 위한 규칙 조건. 이 조건이 없으면 리프노드\n",
    "- gini : 다음의 value=[]로 주어진 데이터 분포에서의 지니 계수이다.\n",
    "- samples : 현 규칙에 해당하는 데이터 건수\n",
    "- value=[]는 클래스 값 기반의 데이터 건수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "[0.025 0.    0.555 0.42 ]\n",
      "sepal length (cm) : 0.025\n",
      "sepal width (cm) : 0.000\n",
      "petal length (cm) : 0.555\n",
      "petal width (cm) : 0.420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit()으로 학습된 df_clf에서 feature_importances_속성을 가져와 피처별로 중요도 값 매핑하고 막대그래프로 표현하기\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# feature importance 추출\n",
    "print(\"Feature importances:\\n{0}\".format(np.round(dt_clf.feature_importances_, 3)))\n",
    "\n",
    "# feature별 importance 매핑\n",
    "# zip : (iterable: 반복가능한 자료형, 여러개 입력 가능하다는 말) 동일한 개수로 이루어진 자료형을 묶어주는 역할을 하는 함수\n",
    "for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n",
    "    print('{0} : {1:.3f}'.format(name, value))\n",
    "\n",
    "# feature importance를 column 별로 시각화하기\n",
    "sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "petal_length가 가장 피처 중요도가 높음을 알 수 있다.\n",
    "\n",
    "이렇게 결정 트리는 알고리즘 자체가 직관적이기 때문에 알고리즘과 관련된 요소를 시각적으로 표현할 수 있는 다양한 방안이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 과적합(Overfitting)\n",
    "결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make_classification() 함수 : 분류를 위한 테스트용 데이터를 쉽게 만들 수 있도록 한다.\n",
    "    - make_classification() 호출 시 반환되는 객체 : 피처 데이터 세트, 클래스 레이블 데이터 세트\n",
    "    - 인자\n",
    "        - n_fatures : 독립변수의 수(전체 피처 수)\n",
    "        - n_redundant : 독립변수 중 다른 독립변수의 선형 조합으로 나타나는 성분의 수\n",
    "        - n_informative : 독립변수 중 종속변수와 상관관계가 있는 성분의 수\n",
    "        - n_classes : 종속변수 클래스 수\n",
    "        - n_clusters_per_class : 클래스명 클러스터의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa1912944c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 분류를 위한 데이터 세트를 임의로 만들기\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.title(\"3 Class values with 2 Features Sample data creation\")\n",
    "\n",
    "# 2차원 시각화를 위해서 피처는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성\n",
    "X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                                          n_classes=3, n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# 그래프 형태로 2개의 피처로 2차원 좌표 시각화, 각 클래스 값은 다른 색깔로 표시된다.\n",
    "plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k')\n",
    "# s : marker 크기 영역 지정(2배 늘리려면 4배 입력해야함. 넓이의 제곱으로 영역의 크기 측정)\n",
    "# c : 색\n",
    "# edgecolor : 마커 테두리선의 색 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 모델이 클래스 값을 예측하는 결정 기준을 색상과 경계로 나타내 모델이 어떻게 데이터 세트를 예측 분류하는지 잘 이해할 수 있게 해주는 함수 만들기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Classifier의 Decision Boundary를 시각화 하는 함수\n",
    "def visualize_boundary(model, X, y):\n",
    "    fig,ax = plt.subplots()\n",
    "    \n",
    "    # 학습 데이타 scatter plot으로 나타내기\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim_start , xlim_end = ax.get_xlim()\n",
    "    ylim_start , ylim_end = ax.get_ylim()\n",
    "    \n",
    "    # 호출 파라미터로 들어온 training 데이타로 model 학습 . \n",
    "    model.fit(X, y)\n",
    "    # meshgrid 형태인 모든 좌표값으로 예측 수행. \n",
    "    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # contourf() 를 이용하여 class boundary 를 visualization 수행. \n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='rainbow', clim=(y.min(), y.max()),\n",
    "                           zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X_features와 y_labels 데이터 세트를 기반으로 결정 트리 학습하기\n",
    "# 첫번째로, 결정 트리 생성에 별다른 제약이 없도록 하이퍼 파라미터가 디폴트인 Classifier를 학습하고 결정 기준 경계를 시각화하기\n",
    "# 결정 트리 모델이 어떠한 결정 기준을 가지고 분할하면서 데이터 분류하는지 확인하기\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화\n",
    "dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)\n",
    "visualize_boundary(dt_clf, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일부 이상치 데이터까지 분류하기 위해 분할이 자주 일어나서 결정 기준 경계가 매우 많아졌다. 결정 트리의 기본 하이퍼 파라미터 설정은 리프 노드 안에 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준으로 인해 결정 기준 경계가 많아지고 복잡해졌다.\n",
    "\n",
    "이렇게 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# min_samples_leaf = 6으로 트리 생성 조건을 제약한 결정 경계 시각화\n",
    "dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)\n",
    "visualize_boundary(dt_clf, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐음을 알 수 있다.\n",
    "\n",
    "다양한 테스트 데이터 세트를 기반으로한 결정 트리 모델의 예측 성능은 두번째 모델(min_samples_leaf=6)이 더 뛰어날 가능성이 높다. 왜냐하면 테스트 데이터 세트는 학습 데이터 세트와는 다른 데이터 세트이므로, 학습 데이터에만 지나치게 최적화된 분류 기준은 오히려 테스트 데이터 세트에서 정확도를 떨어뜨릴 수 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 실습 - 사용자 행동 인식 데이터 세트"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "결정트리를 이용해 UCI Machine Learning Repository 에서 제공하는 사용자 행동 인식 데이터 세트에 대한 예측 분류를 수행해보자.\n",
    "\n",
    "해당 데이터는 30명에게 스마트폰 센서를 장착한 뒤 사람의 동작과 관련된 여러가지 피처를 수집한 데이터이다. 수집된 피처 세트를 기반으로 결정트리를 이용해 어떠한 동작인지 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처명에서 10개만 추출: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.\n",
    "feature_name_df = pd.read_csv('/Users/wizdom/Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "# /s+ 의미 : 1개 이상의 공백으로 구분\n",
    "# header=None : 칼럼 이름 X\n",
    "# names : 불러올때 칼럼 이름을 지정한 이름으로 변경\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출:', feature_name[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처명을 보면 인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, Z축 값으로 돼 있음을 유추할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_index    42\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,32</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              column_index\n",
       "column_name                               \n",
       "fBodyAcc-bandsEnergy()-1,16              3\n",
       "fBodyAcc-bandsEnergy()-1,24              3\n",
       "fBodyAcc-bandsEnergy()-1,8               3\n",
       "fBodyAcc-bandsEnergy()-17,24             3\n",
       "fBodyAcc-bandsEnergy()-17,32             3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features.txt 파일은 중복된 피처명을 가지고 있다. 중복된 피처명 얼마나 있는지 확인\n",
    "\n",
    "feature_dup_df = feature_name_df.groupby('column_name').count()\n",
    "print(feature_dup_df[feature_dup_df['column_index'] > 1].count())\n",
    "feature_dup_df[feature_dup_df['column_index'] > 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복된 피처명들을 이용해 데이터 파일을 데이터 세트 데이터프레임에 로드하면 오류가 발생하므로, 중복된 피처명에 대해 원본 피처명에 _1 또는 _2를 추가로 부여해 변경한 뒤에 이를 이용하여 데이터를 데이터프레임에 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 피처명에 _1또는 _2를 추가로 부여해 새로운 피처명을 가지는 데이터프레임을 반환하는 함수 만들기\n",
    "\n",
    "import numpy as np\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                  columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_human_dataset():\n",
    "    \n",
    "    feature_name_df = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt', sep='\\s+', \n",
    "                                 header = None, names = ['column_index', 'column_name'])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 로딩. 칼럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/X_train.txt', sep='\\s+', names = feature_name)\n",
    "    X_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/X_test.txt', sep='\\s+', names = feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로딩하고 칼럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/y_train.txt', sep='\\s+', header = None, names = ['action'])\n",
    "    y_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/y_test.txt', sep='\\s+', header = None, names = ['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 학습 피처 데이터 셋 info()\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7352 entries, 0 to 7351\n",
      "Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)\n",
      "dtypes: float64(561)\n",
      "memory usage: 31.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('## 학습 피처 데이터 셋 info()')\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 세트는 7352개의 레코드로 561개의 피처를 가지고 있다. 피처가 전부 float 형의 숫자 형이므로 별도의 카테고리 인코딩은 수행할 필요 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-meanFreq()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.074323   \n",
       "1         -0.957686         -0.943068  ...                         0.158075   \n",
       "2         -0.977469         -0.938692  ...                         0.414503   \n",
       "3         -0.989302         -0.938692  ...                         0.404573   \n",
       "4         -0.990441         -0.942469  ...                         0.087753   \n",
       "\n",
       "   fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0                        -0.298676                        -0.710304   \n",
       "1                        -0.595051                        -0.861499   \n",
       "2                        -0.390748                        -0.760104   \n",
       "3                        -0.117290                        -0.482845   \n",
       "4                        -0.351471                        -0.699205   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "2                    -0.118559                              0.177899   \n",
       "3                    -0.036788                             -0.012892   \n",
       "4                     0.123320                              0.122542   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "2                          0.100699                              0.808529   \n",
       "3                          0.640011                             -0.485366   \n",
       "4                          0.693578                             -0.615971   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \n",
       "0             -0.841247              0.179941             -0.058627  \n",
       "1             -0.844788              0.180289             -0.054317  \n",
       "2             -0.848933              0.180637             -0.049118  \n",
       "3             -0.848649              0.181935             -0.047663  \n",
       "4             -0.847865              0.185151             -0.043892  \n",
       "\n",
       "[5 rows x 561 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 많은 칼럼의 대부분이 움직임 위치와 관련된 속성임을 알 수 있다.\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    1407\n",
      "5    1374\n",
      "4    1286\n",
      "1    1226\n",
      "2    1073\n",
      "3     986\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train['action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블 값은 6개 값이고 분포도는 특정 값으로 왜곡되지 않고 비교적 고르게 분포되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정 트리 예측 정확도: 0.8548\n",
      "\n",
      "DecisionTreeClassifier 기본 하이퍼 파라미터:\n",
      " {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 156, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier를 이용해 동작 예측 분류 수행\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예제 반복 시마다 동일한 예측 결과 도출을 위해 dom_state 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))\n",
    "\n",
    "# DecisionTreeClassifier의 하이퍼 파라미터 추출\n",
    "print('\\nDecisionTreeClassifier 기본 하이퍼 파라미터:\\n', dt_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결정 트리의 트리 깊이(Tree Depth)가 예측 정확도에 주는 영향 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최고 평균 정확도 수치: 0.8513\n",
      "GridSearchCV 최적 하이퍼 파라미터:  {'max_depth': 16}\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV를 이용해 사이킷런 결정 트리의 깊이를 조절할 수 있는 하이퍼 파라미터인 max_depth 값을 계속 늘리면서 예측 성능 측정해보기\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth' : [6, 8, 10, 12, 16, 20, 24]\n",
    "}\n",
    "\n",
    "# scoring : 예측 성능 측정할 평가 방법\n",
    "# verbose : log출력의 레벨을 조정(자세한 정도를 제어(높아질수록 자세))\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GridSerachCV 객체의 cv_result_ 속성 : CV세트에 하이퍼 파라미터를 순차적으로 입력했을 때의 성능 수치를 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.850791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.851069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.851209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.844135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>0.851344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>0.849440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_max_depth  mean_test_score\n",
       "0               6         0.850791\n",
       "1               8         0.851069\n",
       "2              10         0.851209\n",
       "3              12         0.844135\n",
       "4              16         0.851344\n",
       "5              20         0.850800\n",
       "6              24         0.849440"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV 객체의 cv_results_ 속성을 데이터 프레임으로 생성\n",
    "cv_results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "\n",
    "# max_depth 파라미터 값과 그때의 테스트 세트, 학습 데이터 세트의 정확도 수치 추출\n",
    "cv_results_df[['param_max_depth', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth가 8일때 정점이고 이를 넘어가면서 정확도가 계속 떨어진다.\n",
    "\n",
    "이처럼 결정 트리는 더 완벽한 규칙을 학습 데이터 세트에 적용하기 위해 노드를 지속적으로 분할하면서 깊이가 깊어지고 더욱 더 복잡한 모델이 된다. 깊어진 트리는 학습 데이터 세트에는 올바른 예측 결과를 가져올지 모르지만, 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 6 정확도: 0.8558\n",
      "max_depth = 8 정확도: 0.8707\n",
      "max_depth = 10 정확도: 0.8673\n",
      "max_depth = 12 정확도: 0.8646\n",
      "max_depth = 16 정확도: 0.8575\n",
      "max_depth = 20 정확도: 0.8548\n",
      "max_depth = 24 정확도: 0.8548\n"
     ]
    }
   ],
   "source": [
    "# 별도의 테스트 데이터 세트에서 max_depth의 변화에 따른 값을 측정해보기\n",
    "\n",
    "max_depths = [6, 8, 10, 12, 16, 20, 24]\n",
    "# max_depth 값을 변화시키면서 그때마다 학습과 테스트 세트에서의 예측 성능 측정\n",
    "for depth in max_depths:\n",
    "    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print('max_depth = {0} 정확도: {1:.4f}'.format(depth, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth가 8일 경우 약 87.07%로 가장 높은 정확도를 나타냈다. 또한 8을 넘어가면서 정확도가 계속 감소하고 있다.\n",
    "\n",
    "이처럼 결정 트리는 깊이가 깊어질수록 과적합의 영향력이 커지므로 하이퍼 파라미터를 이용해 깊이를 제어할 수 있어야 한다. 복잡한 모델보다도 트리 깊이를 낮춘 단순한 모델이 더욱 효과적인 결과를 가져올 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_depth와 min_samples_split을 같이 변경하면서 정확도 성능 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최고 평균 정확도 수치: 0.8549\n",
      "GridSearchCV 최적 하이퍼 파라미터:  {'max_depth': 8, 'min_samples_split': 16}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth' : [8, 12, 16, 20],\n",
    "    'min_samples_split' : [16, 24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정 트리 예측 정확도: 0.8717\n"
     ]
    }
   ],
   "source": [
    "# 별도 분리된 테스트 데이터 세트에 해당 하이퍼 파라미터 적용\n",
    "\n",
    "# best_estimator_ : 최적 하이퍼 파라미터로 학습이 완료된 estimator 객체\n",
    "best_df_clf = grid_cv.best_estimator_\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred1)\n",
    "print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결정트리에서 각 피처의 중요도를 feature_importances_ 속성을 이용해 알아보기\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importances_values = best_df_clf.feature_importances_\n",
    "# Top 중요도로 정렬을 쉽게 하고, 시본의 막대그래프로 쉽게 표현하기 위해 Series 변환\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\n",
    "# 중요도값 순으로 Series를 정렬\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "sns.barplot(x=ftr_top20, y = ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. GBM(Gradient Boosting Machine)\n",
    "### GBM의 개요 및 실습 \n",
    "#### 부스팅의 대표적인 구현\n",
    "- AdaBoost(Adaptive Boosting)\n",
    "    - 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘\n",
    "    - 즉, 약한 학습기가 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측을 수행한다.\n",
    "    - 예를 들어 첫번째 학습기에 가중치 0.3 두번째에는 0.5 세번째에는 0.8을 부여한 후 모두 결합해 예측을 수행한다.\n",
    "    \n",
    "    \n",
    "- GBM(Gradient Boost Machine) \n",
    "    - 에이다부스트와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이이다.\n",
    "        - 오류값 : 실제값 - 예측값\n",
    "        - 경사 하강법 : 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것\n",
    "    - 분류는 물론, 회귀도 가능\n",
    "    - GradientBoostingClassifier 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-cd842b496d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0m\u001b[1;32m    410\u001b[0m                                    dtype=DTYPE, multi_output=True)\n\u001b[1;32m    411\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'"
     ]
    }
   ],
   "source": [
    "# 사용자 행동 데이터 세트 예측 분류\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행시간 측정을 취함. 시작 시간 설정\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간: {0:.1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로, GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다.\n",
    "\n",
    "그러나 수행시간이 오래 걸리고(GBM이 극복해야 할 중요한 과제), 하이퍼 파라미터 튜닝 노력도 더 필요하다.\n",
    "(반면에 랜덤 포레스트는 상대적으로 빠른 수행 시간을 보장해주기 떄문에 더 쉽게 예측 결과 도출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM 하이퍼 파라미터 튜닝\n",
    "- loss : 경사 하강법에서 사용할 비용함수 지정(default : deviance)\n",
    "\n",
    "\n",
    "- learning rage : GBMdㅣ 학습을 진행할 때마다 적용하는 학습률. weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수.\n",
    "    - 0~1 사이 값을 지정할 수 있으며, 기본값은 0.1이다.\n",
    "    - 너무 작은 값 적용할 시 : \n",
    "        - 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성 높다.\n",
    "        - 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다.\n",
    "    - 많은 weak learner :\n",
    "        - 순차적인 반복이 필요해 수행시간이 오래걸린다.\n",
    "        - 최소 오류 값을 찾지 못하고 그냥 지나쳐버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행 가능\n",
    "    - 따라서 이러한 특성때문에 n_estimator와 상호 보완적으로 조합해 사용한다.\n",
    "        - learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측성능이 조금씩 좋아질 수 있으나 시간이 오래걸리고 성능 역시 현격히 좋아지지는 않음\n",
    "\n",
    "\n",
    "- n_estimators : weak learner의 개수\n",
    "    - weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있지만, 개수가 많을 수록 시간 오래걸림\n",
    "    \n",
    "\n",
    "- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율\n",
    "    - 기본값 1, 이는 전체 학습 데이터를 기반으로 학습한다는 의미 \n",
    "    - 과적합이 염려되는 경우 1보다 작은 값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV를 이용해 하이퍼 파라미터를 최적화\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 500],\n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_cv = GridSearchCV(gb_clf, parma_gird=params, cv=2, verbose=1)\n",
    "gird_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 설정 그대로 테스트 데이터 세트에 적용해 에측 정확도 확인\n",
    "\n",
    "# GridSearchCV를 이용해 최적으로 학습된 estimator로 예측 수행\n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. XGBoost(eXtra Gradient Boost)\n",
    ": 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나\n",
    "- GBM에 기반하고 있지만, GBM의 단점인 느린 수행시간 및 과적합 규제 부재 등의 문제를 해결했다.\n",
    "    - 병렬 CPU 환경에서 병렬학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다.\n",
    "    \n",
    "    \n",
    "- **XGBoost 장점**\n",
    "    - 뛰어난 예측 성능 발휘\n",
    "    - GBM 대비 빠른 수행 시간\n",
    "        - XBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 가지고 있다. \n",
    "        - GBM에 비해 수행시간이 빠르다는 것이지 다른 머신러닝 알고리즘에 비해 빠르다는 의미는 아님.\n",
    "    - 과적합 규제\n",
    "        - XGBoost는 자체에 과적합 규제 기능으로 좀 더 강한 내구성을 가진다.\n",
    "    - Tree pruning(나무 가지치기)\n",
    "        - GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조정하기도 하지만, tree pruning으로 더이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.\n",
    "    - 자체 내장된 교차 검증\n",
    "       - 반복 수행시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차검증을 수행해 최적화된 바복 수행 횟수를 가질 수 있다.\n",
    "       - 지정된 반복 횟수가 아닌 교차검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단기능이 있다.\n",
    "           - 조기 중단기능은 수행 속도를 향상시킨다.\n",
    "           - n_estimators에 지정한 부스팅 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선한다.\n",
    "    - 결손값 자체 처리 기능\n",
    "    - 자체적으로 성능 평가, 피처 중요도 등의 시각화 기능을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost의 파이썬 패키지명 : xgboost\n",
    "    - 초기 출시에는 사이킷런과 호환되지 않는 독자적인 XgBoost 전용의 패키지였다.\n",
    "    - 사람들이 사이킷런을 많이 사용하기 때문데 사이킷런과 연동할 수 있는 래퍼 클래스(Wrapper class)를 제공했다.\n",
    "- XGBoost 패키지의 사이킷런 래퍼 클래스 : XGBClassifier, XGBRegressor\n",
    "    - 이를 이용하면 사이킷런 estimator가 학습을 위해 사용하는 fit과 predict과 같은 표준 사이킷런 개발 프로세스 및 다양한 유틸리티 활용 가능\n",
    "- 사이킷런 래퍼 XGBoost 모듈은 사이킷런의 다른 estimator와 사용법이 같은데, 파이썬 네이티브 XGBoost는 고유의 API 하이퍼 파라미터를 이용한다. 크게 다르지는 않으나 몇 가지 주의할 점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 하이퍼 파라미터\n",
    "- 파이썬 래퍼 XGBoost 모듈과 사이킷런 래퍼 XGBoost 모듈의 일부 하이퍼 파라미터는 약간 다르므로 주의가 필요하다.\n",
    "    - 동일한 기능을 의미하는 하이퍼 파라미터지만, 사이킷런 파라미터의 범용화된 이름 규칙에 따라 파라미터 명이 달라짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 일반 파라미터 >\n",
    ": 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터.(디폴트 파라미터 값을 바꾸는 경우는 거의 없다.)\n",
    "- **booster**\n",
    "    - gbtree(tree based model)또는 gblinear(linear model) 선택. \n",
    "        - 디폴트는 gbree\n",
    "- **silent**\n",
    "    - 디폴트는 0, 출력 메시지를 나타내고 싶지 않을 경우 1\n",
    "- **nthread** \n",
    "    - CPU의 실행 스레드 개수 조정\n",
    "        - 디폴트는 CPU의 전체 스레드를 다 사용하는 것.\n",
    "    - 멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해서 ML 애플리케이션을 구동하는 경우에 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 부스터 파라미터 >\n",
    ": 트리 최적화, 부스팅, regularization 등과 관련 파라미터 등을 지칭한다.\n",
    "- **eta[default=0.3, alias:learning_rate]**\n",
    "    - GBM의 학습률(learning_rate)와 같은 파라미터.\n",
    "    - 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값.\n",
    "        - 사이킷런 래퍼 클래스를 이용할경우 eta는 learning_rate 파라미터로 대체되며, 디폴트는 0.1이다. 보통은 0.01 ~ 0.2 사이의 값을 선호한다.\n",
    "        \n",
    "        \n",
    "- **num_boost_rounds**\n",
    "    - GBM의 n_estimator과 같은 파라미터이다.\n",
    "    \n",
    "    \n",
    "- **min_child_weight[default=1]**\n",
    "    - 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합\n",
    "    - 클수록 분할을 자제한다.\n",
    "    - 과적합을 조절하기 위해 사용\n",
    "    \n",
    "    \n",
    "- **gamma[default=0, alias:min_split_loss]**\n",
    "    - 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값.\n",
    "    - 새당 값보다 큰 손실(loss)이 감소된 경우에 리프 노드를 분리한다.\n",
    "    - 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **max_depth[default=6]**\n",
    "    - 트리 기반 알고리즘의 max_depth와 같다.\n",
    "    - 0을 지정하면 깊이 제한 X. 보통은 3~10 사이의 값을 적용한다.\n",
    "    - depth가 높으면 과적합 가능성이 높아진다.\n",
    "    \n",
    "    \n",
    "- **sub_sample[default=1]**\n",
    "    - GBM의 subsample과 동일. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정ㅎ나다.\n",
    "    - 0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는데 사용한다. \n",
    "    - 일반적으로 0.5 ~ 1 사이의 값을 사용한다.\n",
    "    \n",
    "    \n",
    "- **colsample_bytree[default=1]**\n",
    "    - GBM의 max_features와 유사. 트리 생성에 필요한 피처를 임의로 샘플링하는데 사용\n",
    "    - 매우 많은 피처가 있는 경우 과적합을 조정하는데 적용\n",
    "    \n",
    "    \n",
    "- **lambda[default=1, alias:reg_lambda]**\n",
    "    - L2 Regularization 적용 값. 피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **alpha[default=0, alias:reg_alpha]**\n",
    "    - L1 Regularization 적용 값.피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **scale_pos_wight[default=1]**\n",
    "    - 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 학습 태스크 파라미터 >\n",
    "- **objective**\n",
    "    - 최솟값을 가져야 할 손실 수 정의. \n",
    "    - 주로 사용되는 손실 함수는 이진 분류인지 다중 분류인지에 따라 달라진다.\n",
    "    \n",
    "    \n",
    "- **binary:logistic**\n",
    "    - 이진 분류일 때 적용\n",
    "    \n",
    "- **multi:softmax**\n",
    "    - 다중 분류일 때 적용.\n",
    "    - 손실함수가 multi:softmax일 경우에는 레이블 클래스 개수인 num_class 파라미터를 지정해야 한다.\n",
    "\n",
    "\n",
    "- **multi:softpob**\n",
    "    - multi:softmax와 유사하나, 개별 레이블 클래스의 해당되는 예측 확률을 반환한다.\n",
    "\n",
    "\n",
    "- **eval_metric**\n",
    "    - 검증에 사용되는 함수를 정의.\n",
    "    - 기본값은 회귀인 경우는 rmse, 분류일 경우에는 error이다.\n",
    "    - **eval_metirc의 값 유형**\n",
    "        - **rmse** : 평균 제곱근 오차(Root Mean Square Error)\n",
    "            - 잔차의 제곱합을 산술평균한 값의 제곱근. 관측값들간의 상호간 편차\n",
    "        - **mae** : 평균 절대 오차(Mean Absolute Error)\n",
    "            - 오차의 절대값의 평균\n",
    "        - **logloss** : 음의 로그 가능도(Negative log-likelihood)\n",
    "            - 로그가능도 x -1. 확률 변수가 독립확률변수로 나누어지는 경우와 같이 확률분포함수가 곱셈꼴로 나올때 미분계산의 편의성을 위해 사용한다.\n",
    "        - **error** : 이진분류 오류율(Binary classification error rate(0.5 threshold))\n",
    "        - **merror** : 다중 클래스 분류 오류율(Multicalss classification error rate)\n",
    "        - **mlogloss** : 다중클래스의 로그 손실(Multiclass logloss)\n",
    "        - **acu** : 곡선 하위 영역(Area under the curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뛰어난 알고리즘일수록 파라미터를 튜닝할 필요가 적다. 또한 파라미터 튜닝에 들이는 공수 대비 성능 향상 효과가 높지 않은 경우가 대부분이다.\n",
    "\n",
    "파라미터를 튜닝하는 경우의 수는 여러 가지 상황에 따라 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과적합 문제가 심각할 경우 다음과 같이 적용해본다.\n",
    "    - eta값을 낮춘다.(0.01 ~ 0.1) eta값을 낮출 경ㅇ우 num_round(또는 n_estimators)는 반대로 높여줘야함\n",
    "    - max_depth 값을 낮춘다.\n",
    "    - min_child_weight 값을 높인다.\n",
    "    - gamma 값을 높인다.\n",
    "    - subsample과 colsample_bytree를 조정하는 것도 트리가 너무 복잡하게 생성되는 것을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 버전 확인\n",
    "import xgboost \n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 적용 - 위스콘신 유방암 예측\n",
    "위스콘신 유방암 데이터 세트를 활용하여 파이썬 래퍼 XGBoost API의 사용법을 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 및 데이터 로드\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance # 피처의 중요도 시각화\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wanings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_features = dataset.data\n",
    "y_label = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\n",
    "cancer_df['target'] = y_label\n",
    "cancer_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "종양의 크기와 모양에 관련된 많은 속성이 숫자형 값으로 되어 있다.\n",
    "\n",
    "타깃 레이블 값의 종류는 악성인 malignant가 0, 양성인 benign이 1값으로 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 값의 분포 확인\n",
    "\n",
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "print(x_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬 래퍼 XGBoost와 사이킷런의 차이\n",
    "먼저 눈에 띄는 차이는 파이썬 래퍼 XGBoost는 학습용과 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다.\n",
    "- DMatrix : 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트\n",
    "    - 주요 파라미터\n",
    "        - data : 피처 데이터 세트\n",
    "        - label : 분류의 경우에는 레이블 데이터세트, 회귀의 경우는 숫자형인 종솟값 데이터 세트\n",
    "    - DMatrix는 넘파이 이외에 libsvm txt 포맷 파일, xgboost 이진 버퍼 파일을 파라미터로 입력받아 변환할 수 있다. \n",
    "        - 판다스의 데이터프레임으로 데이터 인터페이스를 하기 위해서는 DataFrame.values를 이용해 넘파이로 일차 변환한 뒤에 이를 이용해 DMatrix 변환을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 형태의 학습 데이터 세트와 테스트 데이터 세트를 DMatrix로 변환\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.Dmatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 래퍼 XGBoost 모듈인 xgboost를 이용해 학습을 수행하기 전에, 먼저 XGBoost의 하이퍼 파라미터 설정. 주로 !!딕셔너리 형태!!로 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'max_depth':3,\n",
    "          'eta':0.1,\n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'earl_stoppings':100 # 조기 중단할 수 있는 최소 반복 횟수\n",
    "}\n",
    "num_rounds = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 하이퍼 파라미터를 xgboost 모듈의 train()함수에 파라미터로 전달한다.(사이킷런의 경우는 Estimator의 생성자를 하이퍼 파라미터로 전달)\n",
    "\n",
    "\n",
    "- **early_stopping_rounds 파라미터 입력**을 통해 **조기중단**할 수 있도록 설정한다.\n",
    "    - early_stopping_rounds 파라미터를 설정해 조기 중단을 수행하기 위해서는 반드시 **eval_set**과 **eval_metric**이 함께 설정되어야 한다.\n",
    "        - XGBoost는 반복마다 eval_set으로 지정된 데이터 세트에서 eval_metric의 지정된 평가 지표로 예측 오류를 측정한다.\n",
    "        - **eval_set** : 성능 평가를 수행할 평가용 데이터 세트 설정\n",
    "        - **eval_metric** : 평가 세트에 적용할 성능 평가 방법. 분류일 경우 주로 error, logloss를 적용\n",
    "    - xgboost train()의 **evals 파라미터**에 학습된 데이터 세트와 eval 데이터 세트를 명기해주면, 평가를 eval 데이터 세트에 수행하면서 조기 중단을 적용할 수 있다. 조기 중단을 수행하려면 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명기\n",
    "wlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train()함수의 파라미터로 전달\n",
    "# 반복시마다 evals에 표시된 데이터 세트에 대해 평가지표 결과 출력된다.\n",
    "# train()은 학습이 완료된 모델 객체를 반환\n",
    "xgb_model = xgb.train(params = paras, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=100, evals=wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 train()함수를 호출해 학습이 완료된 모델 객체를 반환하게 되는데, 이 모델 객체는 예측을 위해 predict() 메서드를 이용한다.\n",
    "    - 유의할 점은 사이킷런의 predict()메서드는 예측 결과 클래스 값(즉 0, 1)을 반환하는데 반해 xgboost의 predict()는 예측 결괏값이 아닌 예측 결과를 추정할 수 있는 확률 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict() 수행 결괏값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정해 리스트 객체인 preds에 저장\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시: ', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델의 예측 성능 평가\n",
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xboost의 plot_importance() : 피처의 중요도를 막대그래프 형식으로 나타냄\n",
    "    - 기본 평가 지표로 f1스코어를 기반으로 한다.\n",
    "    - 호출 시 파라미터로 앞에서 학습이 완료된 앞델 객체 및 맷플롯립의 ax 객체를 입력하기만 하면 된다.\n",
    "    - (사이킷런은 estimator 객체의 feature_importances_속성을 이용해 직접 시각화 코드를 작성해야 한다.)\n",
    "    - 유의 할 점\n",
    "        - xgboost 넘파이 기반의 피처 데이터로 학습 시에 피처명을 제대로 알 수가 없으므로 f0, f1와 같이 피처 순서별로 f자 뒤에 숫자를 붙여서 X축에 피처들로 나열한다. (f0이 첫번째 피처)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(xgb_model, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost 모듈의 to_graphviz() API : 이를 이용하면 주피터 노트북에 바로 규칙 트리 구조를 그릴 수 있다.\n",
    "    - Graphviz 프로그램과 패키지가 설치돼 있어야 한다.\n",
    "    - xgboost.to_graphviz()내에 파라미터로 학습이 완료된 모델 객체와 Graphviz가 참조할 파일명을 입력해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv( ) : 파이썬 래퍼 XGBoost에서 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 제공\n",
    "    - **파라미터**\n",
    "        - **params** (dict) : 부스터 파라미터\n",
    "        - **dtrain** (DMatrix) : 학습 데이터\n",
    "        - **num_boost_round** (int) : 부스팅 반복 횟수\n",
    "        - **nfold** (int) : CV 폴드 개수\n",
    "        - **stratified** (bool) : CV 수행 시 층화 표본 추출(stratified sampling) 수행 여부\n",
    "        - **metrics** (string or list of string) : CV 수행 시 모니터링할 성능 평가 지표\n",
    "        - **early_stopping_rounds** (int) : 조기 중단을 활성화시킴. 반복 횟수 지정\n",
    "    \n",
    "    \n",
    "    - 데이터프레임 형태로 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 래퍼 XGBoost의 개요 및 적용\n",
    "- 다른 estimator와 동일하게 fit과 predict만으로 학습과 예측이 가능하고, GridSearchCV, Pipeline등 사이킷런의 다른 유틸리티를 그대로 사용할 수 있다.\n",
    "    - 따라서 기존의 다른 머신러닝 알고리즘으로 만들어놓은 프로그램이 있더라도 알고리즘 클래스만 XGBoost 래퍼 클래스로 바꾸면 기존 클래스 그대로 사용 가능\n",
    "    \n",
    "    \n",
    "- XGBClassifier(분류를 위한 래퍼 클래스), XGBRegressor(회귀를 위한 래퍼 클래스)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBClassifier는 기존 사이킷런에서 일반적으로 사용하는 하이퍼 파라미터와 호환성을 유지하기 위해 파이썬 래퍼 xgboost 모듈에서 사용하던 네이티브 하이퍼 파라미터 몇 개를 다음과 같이 변경했다.\n",
    "    - eta -> learning_rate\n",
    "    - sub_sample -> subsample\n",
    "    - lambda -> reg_lambda\n",
    "    - alpha -> reg_alpha\n",
    "    \n",
    "\n",
    "- xgboost의 n_estimator와 num_boost_round 하이퍼 파라미터는 서로 동일\n",
    "    - 만일 두개가 동시에 사용 되면\n",
    "        - 파이썬 래퍼 XGBoost는 num_boost_round 파라미터 적용\n",
    "        - XGBClassifier와 같은 사이킷런 래퍼 클래스에서는 n_estimator 파라미터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위스콘신 대학병원의 유방암 데이터 세트를 분류를 위한 래퍼 클래스인 XGBClassifier를 이용해 예측\n",
    "\n",
    "# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb_wrapper.fit(X_train, y_trian)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 성능 평가\n",
    "\n",
    "get_clf_eval(y_test, w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런 래퍼 XGBoost에서의 조기 중단 수행\n",
    "    - 조기 중단 관련한 파라미터를 fit()에 입력한다.\n",
    "        - early_stopping_rounds : 평가 지표가 향상될 수 있는 반복 횟수 정의\n",
    "        - eval_metic : 조기 중단을 위한 평가 지표\n",
    "        - eval_set : 성능 평가를 수행할 데이터 세트\n",
    "            - (학습데이터가 아니라 별도의 데이터 세트여야한다. 학습시에는 완전히 알려지지 않은 데이터 세트 사용해야함. 왜냐면 학습 시에 미리 참고가 되어 과적합할 수 있기 때문)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 예제에서는 데이터 세트의 크기가 작아 테스트 데이터를 평가용으로 사용했다. 이부분을 주지\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_esimators=400, learning_rate=0.1, max_depth=3)\n",
    "evals = [(X_test, y_test)]\n",
    "xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "# verbose=True : 부팅시 상세정보를 보여줌으로써, 어느 부분에서 어떤 지연이 있는지 확인 가능\n",
    "ws100_preds = xgb_wrapper.predict(X_test)\n",
    "ws100_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "211번 반복시 logloss가 0.085593이고 311번 반복 시 logloss가 0.085948인데, 211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에 더이상 반복하지 않고 멈추었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 중단으로 학습된 XGBClassifier의 예측성능\n",
    "get_clf_eval(y_test, ws100_preds, ws100_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기 중단을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. \n",
    "\n",
    "아직 성능이 향상될 여지가 있음에도 불구하고 n번 반복하는 동안 성능 평가 지표가 향상되지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측성능이 나빠질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_rounds를 10으로 설정하고 재학습\n",
    "xgb_wrapper.fit(X_train, y_train, earl_stopping_rounds=10, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "ws10_preds = xgb_wrapper.predict(X_test)\n",
    "ws10_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, ws10_preds, ws10_preds_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early_stopping_rounds=100일때보다 정확도가 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 피처의 중요도를 시각화하는 모듈인 plot_importance()에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 도출해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "# 사이킷런 wrapper 클래스를 입력해도 무방\n",
    "plot_importance(xgb_wrapper, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. 분류 실습 - 캐글 산탄데르 고객 만족 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (76020, 371)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "2   4     2     23                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "0                      0.0                      0.0  ...   \n",
       "1                      0.0                      0.0  ...   \n",
       "2                      0.0                      0.0  ...   \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "2                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
       "0                     0.0                     0.0  39205.17       0  \n",
       "1                     0.0                     0.0  49278.03       0  \n",
       "2                     0.0                     0.0  67333.77       0  \n",
       "\n",
       "[3 rows x 371 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "cust_df = pd.read_csv(\"/Users/wizdom/Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/santader_customer_satisfaction/train_santader.csv\",\n",
    "                     encoding='latin-1')\n",
    "print('dataset shape:', cust_df.shape)\n",
    "cust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76020 entries, 0 to 76019\n",
      "Columns: 371 entries, ID to TARGET\n",
      "dtypes: float64(111), int64(260)\n",
      "memory usage: 215.2 MB\n"
     ]
    }
   ],
   "source": [
    "cust_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    73012\n",
      "1     3008\n",
      "Name: TARGET, dtype: int64\n",
      "Unsatisfied 비율은 0.04\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터에서 만족과 불만족의 비율 살펴보기\n",
    "\n",
    "print(cust_df['TARGET'].value_counts())\n",
    "unsatisfied_cnt = cust_df[cust_df['TARGET']==1].TARGET.count()\n",
    "total_cnt = cust_df.TARGET.count()\n",
    "print('Unsatisfied 비율은 {0:.2f}'.format(unsatisfied_cnt / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>7.602000e+04</td>\n",
       "      <td>76020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75964.050723</td>\n",
       "      <td>-1523.199277</td>\n",
       "      <td>33.212865</td>\n",
       "      <td>86.208265</td>\n",
       "      <td>72.363067</td>\n",
       "      <td>119.529632</td>\n",
       "      <td>3.559130</td>\n",
       "      <td>6.472698</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>...</td>\n",
       "      <td>7.935824</td>\n",
       "      <td>1.365146</td>\n",
       "      <td>12.215580</td>\n",
       "      <td>8.784074</td>\n",
       "      <td>31.505324</td>\n",
       "      <td>1.858575</td>\n",
       "      <td>76.026165</td>\n",
       "      <td>56.614351</td>\n",
       "      <td>1.172358e+05</td>\n",
       "      <td>0.039569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43781.947379</td>\n",
       "      <td>39033.462364</td>\n",
       "      <td>12.956486</td>\n",
       "      <td>1614.757313</td>\n",
       "      <td>339.315831</td>\n",
       "      <td>546.266294</td>\n",
       "      <td>93.155749</td>\n",
       "      <td>153.737066</td>\n",
       "      <td>30.604864</td>\n",
       "      <td>36.513513</td>\n",
       "      <td>...</td>\n",
       "      <td>455.887218</td>\n",
       "      <td>113.959637</td>\n",
       "      <td>783.207399</td>\n",
       "      <td>538.439211</td>\n",
       "      <td>2013.125393</td>\n",
       "      <td>147.786584</td>\n",
       "      <td>4040.337842</td>\n",
       "      <td>2852.579397</td>\n",
       "      <td>1.826646e+05</td>\n",
       "      <td>0.194945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.163750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38104.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.787061e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76043.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.064092e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113748.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187563e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151838.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>12888.030000</td>\n",
       "      <td>21024.810000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>11073.570000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50003.880000</td>\n",
       "      <td>20385.720000</td>\n",
       "      <td>138831.630000</td>\n",
       "      <td>91778.730000</td>\n",
       "      <td>438329.220000</td>\n",
       "      <td>24650.010000</td>\n",
       "      <td>681462.900000</td>\n",
       "      <td>397884.300000</td>\n",
       "      <td>2.203474e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID           var3         var15  imp_ent_var16_ult1  \\\n",
       "count   76020.000000   76020.000000  76020.000000        76020.000000   \n",
       "mean    75964.050723   -1523.199277     33.212865           86.208265   \n",
       "std     43781.947379   39033.462364     12.956486         1614.757313   \n",
       "min         1.000000 -999999.000000      5.000000            0.000000   \n",
       "25%     38104.750000       2.000000     23.000000            0.000000   \n",
       "50%     76043.000000       2.000000     28.000000            0.000000   \n",
       "75%    113748.750000       2.000000     40.000000            0.000000   \n",
       "max    151838.000000     238.000000    105.000000       210000.000000   \n",
       "\n",
       "       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 72.363067               119.529632   \n",
       "std                 339.315831               546.266294   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               12888.030000             21024.810000   \n",
       "\n",
       "       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  3.559130                 6.472698   \n",
       "std                  93.155749               153.737066   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max                8237.820000             11073.570000   \n",
       "\n",
       "       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "count             76020.000000             76020.000000  ...   \n",
       "mean                  0.412946                 0.567352  ...   \n",
       "std                  30.604864                36.513513  ...   \n",
       "min                   0.000000                 0.000000  ...   \n",
       "25%                   0.000000                 0.000000  ...   \n",
       "50%                   0.000000                 0.000000  ...   \n",
       "75%                   0.000000                 0.000000  ...   \n",
       "max                6600.000000              6600.000000  ...   \n",
       "\n",
       "       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  7.935824                 1.365146   \n",
       "std                 455.887218               113.959637   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               50003.880000             20385.720000   \n",
       "\n",
       "       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "count            76020.000000            76020.000000   \n",
       "mean                12.215580                8.784074   \n",
       "std                783.207399              538.439211   \n",
       "min                  0.000000                0.000000   \n",
       "25%                  0.000000                0.000000   \n",
       "50%                  0.000000                0.000000   \n",
       "75%                  0.000000                0.000000   \n",
       "max             138831.630000            91778.730000   \n",
       "\n",
       "       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 31.505324                 1.858575   \n",
       "std                2013.125393               147.786584   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max              438329.220000             24650.010000   \n",
       "\n",
       "       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\n",
       "count            76020.000000            76020.000000  7.602000e+04   \n",
       "mean                76.026165               56.614351  1.172358e+05   \n",
       "std               4040.337842             2852.579397  1.826646e+05   \n",
       "min                  0.000000                0.000000  5.163750e+03   \n",
       "25%                  0.000000                0.000000  6.787061e+04   \n",
       "50%                  0.000000                0.000000  1.064092e+05   \n",
       "75%                  0.000000                0.000000  1.187563e+05   \n",
       "max             681462.900000           397884.300000  2.203474e+07   \n",
       "\n",
       "             TARGET  \n",
       "count  76020.000000  \n",
       "mean       0.039569  \n",
       "std        0.194945  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 371 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 피처의 값 분포 간단히 확인\n",
    "\n",
    "cust_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var3 칼럼의 경우 min값이 -999999이다. NaN이나 특정 예외 값을 -999999로 변환했을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2         74165\n",
      " 8           138\n",
      "-999999      116\n",
      " 9           110\n",
      " 3           108\n",
      " 1           105\n",
      " 13           98\n",
      " 7            97\n",
      " 4            86\n",
      " 12           85\n",
      "Name: var3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cust_df.var3.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-999999값이 116개나 있다. var3는 숫자형이고 -999999는 다른 값에 비해 너무 편차가 심하므로, 이를 가장 값이 많은 2로 변환하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처 데이터 shape:(76020, 369)\n"
     ]
    }
   ],
   "source": [
    "cust_df['var3'].replace(-999999, 2, inplace=True)\n",
    "# ID 피처는 단순 식별자에 불과하므로 드롭해주기\n",
    "cust_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# 피처 세트와 레이블 세트 분리. 레이블 칼럼은 데이터프레임의 맨 마지막에 위치하므로 칼럼 위치를 -1로 하여 분리\n",
    "X_features = cust_df.iloc[:, :-1]\n",
    "y_labels = cust_df.iloc[:, -1]\n",
    "print('피처 데이터 shape:{0}'.format(X_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369)\n",
      "\n",
      "학습 세트 레이블 값 분포 비율\n",
      " 0    0.960964\n",
      "1    0.039036\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "테스트 세트 레이블 값 분포 비율\n",
      " 0    0.9583\n",
      "1    0.0417\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 학습과 성능 평가를 위해 학습 데이터 세트와 테스트 데이터 세트 분리\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# 비대칭한 데이터 세트이므로 클래스인 Target 값 분포도가 학습 데이터와 테스트 데이터 세트에 모두 비슷하게 추출됐는지 확인\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "print('\\n학습 세트 레이블 값 분포 비율\\n', y_train.value_counts()/train_cnt)\n",
    "print('\\n테스트 세트 레이블 값 분포 비율\\n', y_test.value_counts()/test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습과 테스트 데이터 세트 모두 TARGET의 값의 분포가 원본 데이터와 유사하게 전체 데이터의 4%정도의 불만족 값으로 만들어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정\n",
    "xgb_clf = XGBClassifier(n_estimators=500, random_state=156)\n",
    "\n",
    "# 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", \n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터 세트를 XGBoost 평가 데이터 세트로 사용하면 과적합의 가능성을 증가시킬 수 있지만, 여기서는 이러한 점만 주지하고 넘어가자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f22961484a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 하이퍼 파라미터 테스트의 수행 속도를 향상 시키기 위해 n_estimators를 100으로 감소(나중에 하이퍼 파라미터 튜닝이 완료되면 다시 증가시킨다.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 칼럼의 개수가 많으므로 과적합 가능성을 가정하고, max_depth, min_child_weight, colsample_bytree 하이퍼 파라미터만 일차 튜닝 대상으로 한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# XGBoost 하이퍼 파라미터 튜닝 수행\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 하이퍼 파라미터 테스트의 수행 속도를 향상 시키기 위해 n_estimators를 100으로 감소(나중에 하이퍼 파라미터 튜닝이 완료되면 다시 증가시킨다.)\n",
    "xgb_clf = XGBClassifier(n_estimators=100)\n",
    "\n",
    "# 칼럼의 개수가 많으므로 과적합 가능성을 가정하고, max_depth, min_child_weight, colsample_bytree 하이퍼 파라미터만 일차 튜닝 대상으로 한다.\n",
    "params = {'max_depth':[5, 7], 'min_child_weight':[1, 3], 'colsample_bytree':[0.5, 0.75]}\n",
    "\n",
    "# cv는 3으로 지정\n",
    "gridcv = GridSearchCV(xgb_clf, param_grid=params, cv=3)\n",
    "# 수행시간이 오래 걸리므로 early_stopping_rounds도 30으로 줄인다.\n",
    "girdcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\", \n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "print('GridSearchCV 최적 파라미터: ', girdcv.best_params_)\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8448로 조금 개선되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 시간이 많이 필요한 ML 모델인 경우, 하이퍼 파라미터 튜닝을 수행하는 요령 중 첫 번째는 먼저 2 ~ 3개 정도의 파라미터를 결합해 최적 하라미터를 찾아낸 뒤에, 이 최적 파라미터를 기반으로 다시 1~2개 파라미터를 결합해 파라미터 튜닝을 수행하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 구한 최적화 하이퍼 파라미터를 기반으로 다른 하이퍼 파라미터를 변경 또는 추가해 다시 최적화 진행\n",
    "\n",
    "# n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함.\n",
    "xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7,\n",
    "                       min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03)\n",
    "\n",
    "# 성능 평가 지표를 auc로, 조기 중단 파라미터 값은 200으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=\"auc\",\n",
    "           eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8456으로 이전 테스트보다 살짝 향상된 결과를 나타내고 있다.\n",
    "- 한가지 아쉬운 점은 XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행 시간이 상당히 더 많이 요구된다는 점이다.\n",
    "    - 그 때문에 하이퍼 파라미터를 다양하게 나열해 파라미터를 튜닝하는 것은 많은 시간이 소모된다.\n",
    "- 앙상블 계열 알고리즘에서 하이퍼 파라미터 튜닝으로 성능 수치 개선이 급격하게 되는 경우는 많지 않다. \n",
    "    - 앙상블 계열 알고리즘은 과적합이나 잡음에 기본적으로 뛰어난 알고리즘이때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝된 모델에서 각 피처의 중요도를 그래프로 나타내기\n",
    "\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 XGBoost 예제 코드에서 만들어진 데이터 세트를 기반으로 학습 수행하고, ROC-AUC 측정\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=True)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV로 좀 더 다양한 하이퍼 파라미터에 대한 튜닝 수행\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 200으로 감소\n",
    "lgbm_clf = LGBMClassifier(n_estimators=200)\n",
    "\n",
    "pramas = {'num_leaves':[32, 64], 'max_depth':[128, 160], 'min_child_samples':[60,100], 'subsample':[0.8, 1]}\n",
    "\n",
    "# cv는 3으로 지정\n",
    "gridcv = GridSearchCV(lgbm_clf, param_grid=params, cv=3)\n",
    "gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\",\n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "print('GridSearchCV 최적 파라미터: ', girdcv.best_params_)\n",
    "lgbm_roc_score = roc_auc_score(y_test, girdcv.predict_proba(X_test)[:, 1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 파라미터를 적용하고 다시 학습해 ROC-AUC 측정 결과 도출\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100, max_depth=128)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=True)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. 분류 실습 - 캐글 신용카드 사기 검출\n",
    "### 언더 샘플링과 오버 샘플링의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 예측 성능의 문제가 발생할 수 있는데, 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생한다. \n",
    "\n",
    "즉, 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면에 정상 레이블을 가지는 데이터 거누는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉽다.\n",
    "- 지도학습에서의 극도로 불균형한 레이블 값 분포로 인한 문제점 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요\n",
    "    - 오버 샘플링(Oversampling) 방법 \n",
    "        - 이상 데이터와 같이 적은 데이터를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법. \n",
    "        - 동일한 데이터를 단순히 증식하는 방법은 과적합이 되기 때문에 의미가 없으므로 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식한다.\n",
    "        - 언더 샘플링보다 오버 샘플링 방식이 예측 성능상 더 유리한 경우가 많아 주로 사용된다.\n",
    "        - 대표적으로 SMOTE(Synthetic Minority Over-sampling Technique) 방법이 있다. 적은 데이터 세트에 있는 개별 데이터들의 K 최근접 이웃(K Nearest Neighbor)을 찾아서 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식\n",
    "        - SMOTE를 구현한 대표적인 파이썬 패키지는 imbalanced-learn\n",
    "    \n",
    "    \n",
    "     - 언더 샘플링(Undersampling) 방법\n",
    "        - 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식\n",
    "        - 정상 레이블 데이터를 이상 레이블 데이터 수준으로 줄여버린 상태에서 학습을 수행하면 과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기 때문에 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용하지 않는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 일차 가공 및 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "card_df = pd.read_csv('/Users/wizdom/Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/creditcard.csv')\n",
    "card_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time 피처는 데이터 생선 관련한 작업용 속성이므로 큰 의미가 없으므로 제거. Amount 피처는 신용카드 트랙잭션 금액을 의미하며, Class는 레이블로서 0의 경우 정상, 1의 경우 사기 트랜잭션이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "card_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 인자로 입력받은 Dataframe을 복사한 뒤 Time 칼럼만 삭제하고 복사된 dataframe 반환\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop('Time', axis = 1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 데이터 가공 후, 학습/테스트 데이터 세트 반환하는 함수\n",
    "def get_train_test_dataset(df=None):\n",
    "    # 인자로 입력된 dataframe의 사전 데이터 가공이 완료된 복사 dataframe 반환\n",
    "    df_copy = get_preprocessed_df(df)\n",
    "    # dataframe의 맨 마지막 칼럼이 레이블, 나머지는 피처들\n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    # train_test_split()으로 학습과 테스트 데이터 분할.\n",
    "    # stratify=y_target으로 Stratified기반 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3,\n",
    "                                                       random_state=0, stratify=y_target)\n",
    "    # 학습과 테스트 데이터 세트 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 레이블 값 비율\n",
      "0    99.827451\n",
      "1     0.172549\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "테스트 데이터 레이블 값 비율\n",
      "0    99.826785\n",
      "1     0.173215\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 세트와 테스트 데이터 세트 레이블 값 비율 확인\n",
    "print('학습 데이터 레이블 값 비율')\n",
    "print(y_train.value_counts() / y_train.shape[0] * 100)\n",
    "print('\\n테스트 데이터 레이블 값 비율')\n",
    "print(y_test.value_counts() / y_test.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 레이블과 테스트 데이터 레이블이 큰 차이 없이 잘 분할되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀를 이용한 신용 카드 사기 여부 예측\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차행렬\n",
      " [[85279    16]\n",
      " [   59    89]]\n",
      "정확도: 0.9991, 정밀도: 0.8476, 재현율: 0.6014, F1: 0.7036, AUC: 0.9547\n"
     ]
    }
   ],
   "source": [
    "# 3장에서 사용한 get_clf_eval() 함수를 이용해 평가 수행\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "\n",
    "def get_clf_eval(y_test, pred, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차행렬\\n', confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))  \n",
    "    \n",
    "get_clf_eval(y_test, lr_pred, lr_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자로 사이킷런의 Estimator객체와 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행\n",
    "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "    model.fit(ftr_train, tgt_train)\n",
    "    pred = model.predict(ftr_test)\n",
    "    pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
    "    get_clf_eval(tgt_test, pred, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2990347be26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LightGBM을 이용한 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 본 데이터 세트는 극도로 불균형한 레이블 값 분포도를 가지고 있으므로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# LightGBM을 이용한 모델\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 본 데이터 세트는 극도로 불균형한 레이블 값 분포도를 가지고 있으므로 \n",
    "# LGBMClassifier 객체 생성 시 boost_from_average=False로 파라미터 설정해야 한다.\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀 : 재현율 0.6014, ROC-AUC 0.9547\n",
    "- LightGBM : 재현율 0.7568, ROC-AUC 0.9797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------- 데이터 재가공 뒤에 모델 다시 테스트 ----------------------\n",
    "### 1. 데이터 분포도 변환 후 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀는 선형 모델이다. 대부분의 선형 모델은 중요 피처들의 값이 정규 분포 형태를 유지하는 것을 선호한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'xtricks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dcdddd5a8ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxtricks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'xtricks'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Amount 피처는 신용카드 사용 금액으로 정상/사기 트랜잭션을 결정하는 매우 중요한 속성일 가능성이 높다.\n",
    "# Amount 피처의 분포도 확인\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.xtricks(range(0, 30000, 1000), rotation=60)\n",
    "sns.distplot(card_df['Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카드 사용금액이 1000불 이하인 데이터가 대부분이며, 27000불까지 드물지만 많은 금액을 사용한 경우가 발생하면서 꼬리가 긴 형태의 분포 곡선을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount를 표준 정규 분포 형태로 변환한 뒤에 로지스틱 회귀의 예측 성능을 측정\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 사이킷런의 StandardScaler을 이용해 정규 분포 형태로 Amount 피처값 변환하는 로직으로 수정\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    amount_f = scaler.fit_trainsform(df_copy['Amount'].values.reshape(-1,1))\n",
    "    # 변환된 Amount를 Amount_Scaled로 피처명 변경 후 DataFrame 맨 앞 칼럼으로 입력\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # 기존 Time, Amount 피처 삭제\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "lr_clf = LogisticRegression()\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('### LightGBM 예측 성능 ###')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 모델 모두 변환 이전과 비교해 성능이 크게 개선되지는 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 로그 변환 수행\n",
    "로그 변환은 데이터 분포가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중 하나.\n",
    "\n",
    "원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 변환하기 때문에 데이터 분포도의 왜곡 상당 수준을 개ㅓㄴ해준다.\n",
    "- log1p() 함수를 이용해 간단히 변환 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가공함수 get_preprocessed_df()를 로그 변환 로직으로 변경\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    # 넘파이의 log1p()를 이용해 Amount를 로그 변환\n",
    "    amoung_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('### LightGBM 예측 성능 ###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이상치 데이터 제거 후 모델 학습/예측/평가\n",
    "- 이상치 데이터(Outlier) : 전체 데이터의 패턴에서 벗어난 이상 값을 가진 데이터. 이상치로 인해 머신러닝 모델의 성능에 영향을 받는 경우가 발생하기 쉽다.\n",
    "- 이상치를 찾는 방법은 여러가지가 있지만, 이 중에서 IQR(Inter Quantile Range)방식을 적용한다.\n",
    "    - IQR은 사분위(Quantile)값의 편차를 이용하는 기법으로 흔히 박스플롯 방식으로 시각화 할 수 있다.\n",
    "        - 사분위는 전체 데이터 값이 높은 순으로 정렬하고 이를 1/4(25%)씩으로 구간을 분할하는 것을 지칭한다.\n",
    "        - 25% 구간인 Q1 ~ 75% 구간인 Q3 범위를 IQR이라고 한다.\n",
    "        \n",
    "        \n",
    "    - IQR을 이용해 이상치 데이터를 검출하는 방식 : IQR x 1.5 하여 생성된 범위를 이용해 최댓값과 최솟값을 결정한 뒤, 최댓값을 초과하거나 최솟값에 미달하는 데이터를 이상치로 간주.\n",
    "        - 3/4분위수(Q3)에 IQR x 1.5를 더해서 일반적인 데이터가 가질 수 있는 최댓값으로 가정하고, 1/4분위수(Q1)에 IQR x 1.5를 빼서 일반적인 데이터가 가질 수 있는 최솟값으로 가정\n",
    "        - 경우에 따라서 1.5가 아닌 다른 값을 적용할 수 있다. 보통은 1.5 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상치 데이터를 제거하기 전에, 어떤 피처의 이상치 데이터를 검출할 것인지 선책이 필요하다. 매우 많은 피처가 있을 경우 이들 중 결정값(즉 레이블)과 가장 상관성이 높은 피처들을 위주로 이상치를 검출하는 것이 좋다.\n",
    "(모든 피처들의 이상치를 검출하는 것은 시간이 많이 소모되며, 결정값과 상관성이 높지 않은 피처들의 경우는 이상치를 제거하더라도 크게 성능 향상에 기여하지 않기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 피처별로 상관도를 구한뒤 시각화해보기\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "corr = card_df.corr() # 피처별 상관도 구하기 \n",
    "\n",
    "# heatmap을 통한 시각화\n",
    "# cmap='RdBu'로 설정하면 양의 상관관계가 높을수록 색깔이 진한 파란색에 가까우며, 음에 상관관계가 높을 수록 색깔이 진한 빨간색에 가깝게 표현된다.\n",
    "sns.heatmap(corr, cmap='RdBu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상관관계 히트맵에서 맨 아래 위치한 결정 레이블인 class 피처와 음의 상관관계가 가장 높은 피처는 V14와 V17이다.\n",
    "\n",
    "이 중 V14에 대해서만 이상치를 찾아서 제거해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR을 이용해 이상치를 검출하는 함수 행성\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 인자로 dataframe과 이상치를 검출한 칼럼을 입력받는다. \n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    # fraud에 해당하는 column데이터만 추출, 1/4분위와 3/4분위 지점을 np.percentile로 구함.\n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    # IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최솟값 지점 구함\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    # 최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index 반환\n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_outlier 함수를 이용해 V14칼럼에서 이상치 데이터 찾기\n",
    "\n",
    "outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n",
    "print('이상치 데이터 인덱스: ', outlier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_processed_df()를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경.\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    # 이상치 데이터 삭제하는 로직 추가\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('\\n### LightGBM 예측 성능 ###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상치 데이터를 제거한 뒤, 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가\n",
    "- SMOTE를 적용할 떄는 반드시 학습 데이터 세트만 오버 샘플링을 해야한다.\n",
    "    - 검증 데이터 세트나 테스트 데이터 세트를 오버 샘플링할 경우, 결국은 원본 데이터 세트가 아닌 데이터 세트에서 검증 또는 테스트를 수행하기 떄문에 올바른 검증/테스트가 될 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞 예제에서 생성한 학습 피처/레이블 데이터를 SMOTE 객체의 fit_sample() 메서드를 이용해 증식 한 뒤 데이터를 증식 전과 비교f\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over, y_train_over = smote.fit_sample(X_train, y_train)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shpae, y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포:\\n', pd.Series(y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 세트는 SMOTE 적용 후 2배에 가까운 398,040건으로 데이터가 증식 되었다. \n",
    "\n",
    "그리고 레이블 값이 0과 1의 분포가 동일하게 199,020건으로 생성되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE를 통해 생성된 학습 데이터 세트를 기반으로 로지스틱 회귀 모델 학습 한 뒤 성능 평가하기\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "# ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의\n",
    "get_model_train_eval(lr_clf, ftr_trian=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀 모델의 경우 SMOTE로 오버 샘플링된 데이터로 학습할 경우 재현율이 92.47%로 크게 증가하지만, 반대로 정밀도가 5.4%로 급격하게 저하된다. 재현율이 높더라도 이 정도로 저조한 정밀도로는 현실 업무에 적용할 수 없다.\n",
    "\n",
    "이는 로지스틱 회귀 모델이 오버 샘플링으로 인해 실제 원본 데이터의 유형보다 너무나 많은 class=1 데이터를 학습하면서 실제 테스트 데이터 세트에서 예측을 지나치게 class=1로 적용해 정밀도가 급격히 떨어지게 된 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 결정 임곗값에 따른 정밀도와 재현율 곡선을 통해해 SMOTE로 학습된 로지스틱 회귀모델에 어떠한 문제 발생하고 있는지 시각화\n",
    "# 3장에서 사용한 precision_recall_curve_plot() 함수 사용\n",
    "\n",
    "def precision_recall_curve_plot(y_test, pred_proba_c1):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n",
    "    \n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임계값이 0.99이하에서는 재현율이 매우 좋고 정밀도가 극단적으로 낮다가, 0.99이상에서는 반대로 재현율이 대폭 떨어지고 정밀도가 높아진다.\n",
    "\n",
    "분류 결정 임계값을 조정하더라도 임계값의 민감도가 너무 심해 올바른 재현율/정밀도 성능을 얻을 수 없으므로 로지스틱 회귀 모델의 경우 SMOTE 적용 후 올바른 예측 모델이 생성되지 못했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LighrGBM 모델을 SMOTE로 오버샘플링된 데이터 세트로 학습/예측/평가 수행\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_trian=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재현율이 이상치만 제거한 경우인 82.88%보다 높은 84.93%이 되었지만 정밀도는 이전보다 낮은 93.23%이다.\n",
    "\n",
    "SMOTE를 적용하면 재현율은 높아지나, 정밀도는 낮아지는 것이 일반적이다. 좋은 SMOTE 패키지일수록 재현율 증가율은 높이고 정밀도 감소율은 낮출 수 있도록 효과적으로 데이터를 증식한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 스태킹 앙상블\n",
    "- 스태킹(Stacking)은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅 및 부스팅과 공통점을 가지고 있다.\n",
    "    - 배깅 및 부스팅과의 가장 큰 차이점 : 스태킹은 개별 알고리즘으로 예측한 데이터를 기반으로 ㄷㅏ시 예측을 수행한다는 것.\n",
    "- 즉, 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식.(이렇게 개별 보델의 예측된 데이터 세트를 다시 기반으로 하여 학습하고 예측하는 방식을 메타모델이라 한다.)\n",
    "- 스태킹 모델은 두 종류의 모델이 필요하다.\n",
    "    - 첫 번째는 개별적인 기반 모델\n",
    "    - 두 번째는 이 개별 기반 모델의 에측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델이다.\n",
    "- 스태킹 모델의 핵심 : 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해 최종 메타모델의 학습용 피처 데이터 세트와 테스트용 피처 데이터 세트를 만드는 것.\n",
    "- 스태킹을 적용할 때는 많은 개별 모델이 필요하다. 2~3개의 개별 모델만을 결합해서는 쉽게 예측 성능을 향상시킬 수 없으며 , 스태킹을 적용한다고 해서 반드시 성능 향상이 되리라는 보장도 없다. 일반적으로 성능이 비슷한 모델을 결합해 좀 더 나은 성능 향상을 도출하기 위해 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 스태킹 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_data = cancer_data.data\n",
    "y_label = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스태킹에 사용될 머신러닝 알고리즘 클래스 생성\n",
    "\n",
    "# 개별 ML 모델 생성\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# 스태킹으로 만들어진 데이터 세트를 학습, 예측할 최종 모델\n",
    "lr_final = LogisticRegression(C=10)\n",
    "\n",
    "# 개별 모델들을 학습\n",
    "knn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 정확도: 0.9211\n",
      "랜덤포레스트 정확도: 0.9649\n",
      "결정 트리 정확도: 0.9123\n",
      "에이다부스트 정확도: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# 학습된 개별 모델들이 각자 반환하는 예측 데이터 세트를 생성하고 개별 모델의 정확도 측정.\n",
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "\n",
    "print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\n",
    "print('랜덤포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
    "print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
    "print('에이다부스트 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 114)\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "# 개별 알고리즘으로부터 예측된 예측값을 칼럼 레벨로 옆에 붙여서 피처값으로 만든 뒤, \n",
    "# 최종 메타 모델인 로지스틱 회귀에서 학습데이터로 다시 사용.\n",
    "\n",
    "# 반환된 예측 데이터 세트는 1차원 형태의 ndarray이므로 먼저 반환된 예측 결과를 행 형태로 붙이기\n",
    "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
    "print(pred.shape)\n",
    "\n",
    "# transpose를 이용해 행과 열의 위치 교환. 칼럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦\n",
    "pred = np.transpose(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# 최종 메타 모델인 로지스틱 회귀를 학습하고 예측 정확도 측정\n",
    "\n",
    "lr_final.fit(pred, y_test)\n",
    "final = lr_final.predict(pred)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 모델 정확도보다 향상되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 세트 기반의 스태킹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수.\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n",
    "    # 지정된 n_fols값으로 KFold 생성\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n",
    "    # 추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    print(model.__class__.__name__, 'model 시작')\n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        # 입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출\n",
    "        print('\\t 폴드 세트: ', folder_counter, ' 시작 ')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "        \n",
    "        # 폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n",
    "        model.fit(X_tr, y_tr)\n",
    "        # 폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n",
    "        # 입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장.\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "        print(train_fold_pred)\n",
    "        print(test_pred)\n",
    "        \n",
    "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "    # train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
    "    return train_fold_pred, test_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier model 시작\n",
      "\t 폴드 세트:  0  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "\t 폴드 세트:  1  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]]\n",
      "\t 폴드 세트:  2  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]]\n",
      "\t 폴드 세트:  3  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]]\n",
      "\t 폴드 세트:  4  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]]\n",
      "\t 폴드 세트:  5  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0.]]\n",
      "\t 폴드 세트:  6  시작 \n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 분류 모델별로 stack_base_model()함수 수행\n",
    "knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\n",
    "# rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\n",
    "# dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\n",
    "# ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 학습 피처 데이터 Shape:  (455, 30) 원본 테스트 피처 Shape:  (114, 30)\n",
      "스태킹 학습 피처 데이터 Shape:  (455, 4) 스태킹 테스트 피처 데이터 Shape:  (114, 4)\n"
     ]
    }
   ],
   "source": [
    "# 스텝 2 구현\n",
    "# get_stacking_base_datasets() 호출로 반환된 각 모델별 학습 데이터와 테스트 데이터를 합치기\n",
    "\n",
    "# 넘파이의 concatenate()를 이용해 쉽게 이와 같은 기능을 수행한다. 여러개의 넘파이 배열을 칼럼 또는 로우 레벨로 합쳐주는 기능 제공\n",
    "Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n",
    "print('원본 학습 피처 데이터 Shape: ', X_train.shape, '원본 테스트 피처 Shape: ', X_test.shape)\n",
    "print('스태킹 학습 피처 데이터 Shape: ', Stack_final_X_train.shape,\n",
    "     '스태킹 테스트 피처 데이터 Shape: ', Stack_final_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# 최종 메타 모델인 로지스틱 회귀를 스태킹된 학습용 피처 데이터 세트와 원본 학습 레이블 데이터로 학습한 후,\n",
    "# 스태킹된 테스트 데이터 세트로 예측하고, 예측 결과를 원본 테스트 레이블 데이터와 비교해 정확도 측정\n",
    "\n",
    "lr_final.fit(Stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(Stack_final_X_test)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3e293cf87eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kf' is not defined"
     ]
    }
   ],
   "source": [
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
