{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 04. 분류\n",
    "## 01. 분류(Classification)의 개요\n",
    "- 지도학습 : 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝의 방식\n",
    "\n",
    "\n",
    "- 분류(Classification) : 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측\n",
    "    - 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것.\n",
    "    - 지도학습의 대표적인 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류의 머신러닝 알고리즘\n",
    "    - 베이즈 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)\n",
    "    - 독립변수와 종속변수 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)\n",
    "    - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)\n",
    "    - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)\n",
    "    - 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘\n",
    "    - 심층 연결 기반의 신경망(Neural Network)\n",
    "    - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)\n",
    "        - 분류에서 가장 각광을 받는 방법 중 하나. 정형 데이터의 예측 분석 영역에서 매우 높은 예측 성능으로 인해 애용되고 있다.\n",
    "        - 서로 다른(또는 같은) 알고리즘을 단순히 결합한 형태도 있으나, 배깅과 부스팅 방식이 일반적이다.\n",
    "            - **배깅 방식** : 대표적으로 랜덤 포레스트가 있다.(앙상블의 기본 알고리즘으로, 일반적으로 사용함) 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등으로 많은 분석가가 애용하는 알고리즘이다.\n",
    "            - **부스팅 방식** : 근래의 앙상블 방법은 부스팅 방식으로 지속해서 발전하고 있다. 그래디언 부스팅의 경우 계속적으로 발전된 알고리즘이 등장하면서 정형 데이터의 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡았다.\n",
    "        - **결정 트리와 앙상블**\n",
    "            - 결정 트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘이다. 또한 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적다.\n",
    "                - 하지만 예측 성능을 향상시키기 위해 복잡한 규칙구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점이 있다.(하지만 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용)\n",
    "            - 앙상블은 매우 많은 여러개의 약한 학습기(즉, 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성을을 향상시키는데, 결정트리가 좋은 약한 학습기가 되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.결정트리\n",
    ": 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것. ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘.\n",
    "- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다.\n",
    "- 결정 트리의 구조\n",
    "    - 규칙 노드(Decision Node)로 표시된 노드 -> 규칙 조건\n",
    "    - 리프 노드(Leaf Node)로 표시된 노드 -> 결정된 클래스 값\n",
    "    - 서브 트리(Sub Tree) : 새로운 규칙 조건마다 생성됨\n",
    "- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 규칙노드가 만들어진다. \n",
    "- 하지만, 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 이야기이고, 이는 곧 과적합으로 이어진다. \n",
    "- 즉, 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면\n",
    ": 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 한다.\n",
    "- 이를 위해서는 어떻게 트리를 분할할 것인가가 중요. **최대한 균일한 데이터 세트를 구성**할 수 있도록 분할하는 것이 필요\n",
    "    - 데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미친다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결정 노드\n",
    ": 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듭니다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. \n",
    "##### 정보의 균일도를 측정하는 대표적인 방법 : 엔트로피를 이용한 정보이득 지수, 지니 계수\n",
    "- **정보이득** : 엔트로피라는 개념을 기반으로 한다. 1-엔트로피 지수. 결정트리는 이 정보 이득 지수로 분할 기준을 정한다. 즉, 정보 이득이 높은 속성을 기준으로 분할\n",
    "    - 엔트로피 : 주어진 데이터 집합의 혼잡도. 서로 다른 값이 섞여있으면 엔트로피가 높다.\n",
    "- **지니 계수**  : 불평등 지수를 나타낼 때 사용하는 계수이다. 0이 가장 평등 1로 갈수록 불평등. 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할한다.\n",
    "    - DecisionTreeClassifier는 기본으로 지니계수를 이용해 데이터 세트를 분할한다.\n",
    "    \n",
    "\n",
    "- 결정 트리의 일반적인 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 모델의 특징\n",
    "#### 장점 \n",
    ": 정보의 균일도라는 룰을 기반으로 하고 있어서 알고리즘이 쉽고 직관적이다. 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지안핟. \n",
    "#### 단점\n",
    ": 과적합으로 정확도가 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리 파라미터\n",
    "-  DecisionTreeClassfier  : 분류를 위한 클래스\n",
    "- DecisionTreeRegressor : 회귀를 위한 클래스\n",
    "- 사이킷런의 결정 트리 구현은 CART(Classification And Regression Trees) 알고리즘 기반이다. 이는 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘이다.\n",
    "\n",
    "#### 파라미터\n",
    "- **min_samples_split** : 노드를 분할하기 위한 최소한의 샘플 데이터 수. 과적합을 제어하는데 사용\n",
    "    - 디폴트 2. 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가.\n",
    "- **min_samples_leaf** : 말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수. \n",
    "    - 과적합 제어 용도. \n",
    "    - 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요\n",
    "- **max_features** : 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행\n",
    "    - int: 대상피처의 개수 / float: 전체 피처 중 대상 피처의 퍼센트 / sqrt: 전체 피처 중 sqrt(전체피처개수) / auto: sqrt와 동일 / log: 전체 피처 중 log2(전체피처개수) 선정 \n",
    "- **max_depth** : 트리의 최대 깊이를 규정\n",
    "    - 디폴트는 None. 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 떄까지 계속 깊이를 증가시킴\n",
    "    - 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합 할 수 있으므로 적절한 값으로 제어 필요\n",
    "- **max_leaf_nodes** : 말단 노드(Leaf)의 최대 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처명에서 10개만 추출: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.\n",
    "feature_name_df = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출:', feature_name[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**중복된 피처명을 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_index    42\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,24</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fBodyAcc-bandsEnergy()-17,32</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              column_index\n",
       "column_name                               \n",
       "fBodyAcc-bandsEnergy()-1,16              3\n",
       "fBodyAcc-bandsEnergy()-1,24              3\n",
       "fBodyAcc-bandsEnergy()-1,8               3\n",
       "fBodyAcc-bandsEnergy()-17,24             3\n",
       "fBodyAcc-bandsEnergy()-17,32             3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dup_df = feature_name_df.groupby('column_name').count()\n",
    "print(feature_dup_df[feature_dup_df['column_index'] > 1].count())\n",
    "feature_dup_df[feature_dup_df['column_index'] > 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                  columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                  columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] =  new_feature_name_df['column_name'].astype('U32')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_human_dataset():\n",
    "    \n",
    "    feature_name_df = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/features.txt', sep='/s+', \n",
    "                                 header = None, names = ['column_index', 'column_name'])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 로딩. 칼럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/X_train.txt', sep='/s+', names = feature_name)\n",
    "    X_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/X_test.txt', sep='/s+', names = feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로딩하고 칼럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/train/y_train.txt', sep='/s+', header = None, names = ['action'])\n",
    "    y_test = pd.read_csv('./Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/human_activity/test/y_test.txt', sep='/s+', header = None, names = ['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. GBM(Gradient Boosting Machine)\n",
    "### GBM의 개요 및 실습 \n",
    "#### 부스팅의 대표적인 구현\n",
    "- AdaBoost(Adaptive Boosting)\n",
    "    - 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘\n",
    "    - 즉, 약한 학습기가 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측을 수행한다.\n",
    "    - 예를 들어 첫번째 학습기에 가중치 0.3 두번째에는 0.5 세번째에는 0.8을 부여한 후 모두 결합해 예측을 수행한다.\n",
    "    \n",
    "    \n",
    "- GBM(Gradient Boost Machine) \n",
    "    - 에이다부스트와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이이다.\n",
    "        - 오류값 : 실제값 - 예측값\n",
    "        - 경사 하강법 : 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것\n",
    "    - 분류는 물론, 회귀도 가능\n",
    "    - GradientBoostingClassifier 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-cd842b496d03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgb_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0m\u001b[1;32m    410\u001b[0m                                    dtype=DTYPE, multi_output=True)\n\u001b[1;32m    411\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2.8858451e-001 -2.0294171e-002 -1.3290514e-001 -9.9527860e-001 -9.8311061e-001 -9.1352645e-001 -9.9511208e-001 -9.8318457e-001 -9.2352702e-001 -9.3472378e-001 -5.6737807e-001 -7.4441253e-001  8.5294738e-001  6.8584458e-001  8.1426278e-001 -9.6552279e-001 -9.9994465e-001 -9.9986303e-001 -9.9461218e-001 -9.9423081e-001 -9.8761392e-001 -9.4321999e-001 -4.0774707e-001 -6.7933751e-001 -6.0212187e-001  9.2929351e-001 -8.5301114e-001  3.5990976e-001 -5.8526382e-002  2.5689154e-001 -2.2484763e-001  2.6410572e-001 -9.5245630e-002  2.7885143e-001 -4.6508457e-001  4.9193596e-001 -1.9088356e-001  3.7631389e-001  4.3512919e-001  6.6079033e-001  9.6339614e-001 -1.4083968e-001  1.1537494e-001 -9.8524969e-001 -9.8170843e-001 -8.7762497e-001 -9.8500137e-001 -9.8441622e-001 -8.9467735e-001  8.9205451e-001 -1.6126549e-001  1.2465977e-001  9.7743631e-001 -1.2321341e-001  5.6482734e-002 -3.7542596e-001  8.9946864e-001 -9.7090521e-001 -9.7551037e-001 -9.8432539e-001 -9.8884915e-001 -9.1774264e-001 -1.0000000e+000 -1.0000000e+000  1.1380614e-001 -5.9042500e-001  5.9114630e-001 -5.9177346e-001  5.9246928e-001 -7.4544878e-001  7.2086167e-001 -7.1237239e-001  7.1130003e-001 -9.9511159e-001  9.9567491e-001 -9.9566759e-001  9.9165268e-001  5.7022164e-001  4.3902735e-001  9.8691312e-001  7.7996345e-002  5.0008031e-003 -6.7830808e-002 -9.9351906e-001 -9.8835999e-001 -9.9357497e-001 -9.9448763e-001 -9.8620664e-001 -9.9281835e-001 -9.8518010e-001 -9.9199423e-001 -9.9311887e-001  9.8983471e-001  9.9195686e-001  9.9051920e-001 -9.9352201e-001 -9.9993487e-001 -9.9982045e-001 -9.9987846e-001 -9.9436404e-001 -9.8602487e-001 -9.8923361e-001 -8.1994925e-001 -7.9304645e-001 -8.8885295e-001  1.0000000e+000 -2.2074703e-001  6.3683075e-001  3.8764356e-001  2.4140146e-001 -5.2252848e-002  2.6417720e-001  3.7343945e-001  3.4177752e-001 -5.6979119e-001  2.6539882e-001 -4.7787489e-001 -3.8530050e-001  3.3643943e-002 -1.2651082e-001 -6.1008489e-003 -3.1364791e-002  1.0772540e-001 -9.8531027e-001 -9.7662344e-001 -9.9220528e-001 -9.8458626e-001 -9.7635262e-001 -9.9236164e-001 -8.6704374e-001 -9.3378602e-001 -7.4756618e-001  8.4730796e-001  9.1489534e-001  8.3084054e-001 -9.6718428e-001 -9.9957831e-001 -9.9935432e-001 -9.9976339e-001 -9.8343808e-001 -9.7861401e-001 -9.9296558e-001  8.2631682e-002  2.0226765e-001 -1.6875669e-001  9.6323236e-002 -2.7498511e-001  4.9864419e-001 -2.2031685e-001  1.0000000e+000 -9.7297139e-001  3.1665451e-001  3.7572641e-001  7.2339919e-001 -7.7111201e-001  6.9021323e-001 -3.3183104e-001  7.0958377e-001  1.3487336e-001  3.0109948e-001 -9.9167400e-002 -5.5517369e-002 -6.1985797e-002 -9.9211067e-001 -9.9251927e-001 -9.9205528e-001 -9.9216475e-001 -9.9494156e-001 -9.9261905e-001 -9.9015585e-001 -9.8674277e-001 -9.9204155e-001  9.9442876e-001  9.9175581e-001  9.8935195e-001 -9.9445335e-001 -9.9993755e-001 -9.9995350e-001 -9.9992294e-001 -9.9229974e-001 -9.9693892e-001 -9.9224298e-001 -5.8985096e-001 -6.8845905e-001 -5.7210686e-001  2.9237634e-001 -3.6199802e-001  4.0554269e-001 -3.9006951e-002  9.8928381e-001 -4.1456048e-001  3.9160251e-001  2.8225087e-001  9.2726984e-001 -5.7237001e-001  6.9161920e-001  4.6828982e-001 -1.3107697e-001 -8.7159695e-002  3.3624748e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.5943388e-001 -9.5055150e-001 -9.5799295e-001 -9.4630524e-001 -9.9255572e-001 -9.5943388e-001 -9.9849285e-001 -9.5763740e-001 -2.3258164e-001 -1.7317874e-001 -2.2896660e-002  9.4831568e-002  1.9181715e-001 -9.9330586e-001 -9.9433641e-001 -9.9450037e-001 -9.9278399e-001 -9.9120847e-001 -9.9330586e-001 -9.9989188e-001 -9.9293370e-001 -8.6341476e-001  2.8308522e-001 -2.3730869e-001 -1.0543219e-001 -3.8212313e-002 -9.6895908e-001 -9.6433518e-001 -9.5724477e-001 -9.7505986e-001 -9.9155366e-001 -9.6895908e-001 -9.9928646e-001 -9.4976582e-001  7.2579035e-002  5.7251142e-001 -7.3860219e-001  2.1257776e-001  4.3340495e-001 -9.9424782e-001 -9.9136761e-001 -9.9314298e-001 -9.8893563e-001 -9.9348603e-001 -9.9424782e-001 -9.9994898e-001 -9.9454718e-001 -6.1976763e-001  2.9284049e-001 -1.7688920e-001 -1.4577921e-001 -1.2407233e-001 -9.9478319e-001 -9.8298410e-001 -9.3926865e-001 -9.9542175e-001 -9.8313297e-001 -9.0616498e-001 -9.9688864e-001 -9.8451927e-001 -9.3208200e-001 -9.9375634e-001 -9.8316285e-001 -8.8505422e-001 -9.9396185e-001 -9.9344611e-001 -9.2342772e-001 -9.7473271e-001 -9.9996838e-001 -9.9968911e-001 -9.9489148e-001 -9.9592602e-001 -9.8970889e-001 -9.8799115e-001 -9.4635692e-001 -9.0474776e-001 -5.9130248e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  2.5248290e-001  1.3183575e-001 -5.2050251e-002  1.4205056e-001 -1.5068250e-001 -2.2054694e-001 -5.5873853e-001  2.4676868e-001 -7.4155206e-003 -9.9996279e-001 -9.9998650e-001 -9.9997907e-001 -9.9996244e-001 -9.9993222e-001 -9.9972512e-001 -9.9967039e-001 -9.9998582e-001 -9.9996867e-001 -9.9997686e-001 -9.9986966e-001 -9.9977613e-001 -9.9997115e-001 -9.9991925e-001 -9.9965680e-001 -9.9986046e-001 -9.9986695e-001 -9.9986301e-001 -9.9973783e-001 -9.9973220e-001 -9.9949261e-001 -9.9981364e-001 -9.9968182e-001 -9.9983940e-001 -9.9973823e-001 -9.9961197e-001 -9.9968721e-001 -9.9983863e-001 -9.9359234e-001 -9.9947584e-001 -9.9966204e-001 -9.9964230e-001 -9.9929341e-001 -9.9789222e-001 -9.9593249e-001 -9.9514642e-001 -9.9473990e-001 -9.9968826e-001 -9.9892456e-001 -9.9567134e-001 -9.9487731e-001 -9.9945439e-001 -9.9233245e-001 -9.8716991e-001 -9.8969609e-001 -9.9582068e-001 -9.9093631e-001 -9.9705167e-001 -9.9380547e-001 -9.9051869e-001 -9.9699279e-001 -9.9673689e-001 -9.9197516e-001 -9.9324167e-001 -9.9834907e-001 -9.9110842e-001 -9.5988537e-001 -9.9051499e-001 -9.9993475e-001 -9.9982048e-001 -9.9988449e-001 -9.9302626e-001 -9.9137339e-001 -9.9623962e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000  1.0000000e+000 -2.4000000e-001 -1.0000000e+000  8.7038451e-001  2.1069700e-001  2.6370789e-001 -7.0368577e-001 -9.0374251e-001 -5.8257362e-001 -9.3631005e-001 -5.0734474e-001 -8.0553591e-001 -9.9998649e-001 -9.9997960e-001 -9.9997478e-001 -9.9995513e-001 -9.9991861e-001 -9.9964011e-001 -9.9948330e-001 -9.9996087e-001 -9.9998227e-001 -9.9997072e-001 -9.9981098e-001 -9.9948472e-001 -9.9998083e-001 -9.9985189e-001 -9.9993261e-001 -9.9989993e-001 -9.9982444e-001 -9.9985982e-001 -9.9972751e-001 -9.9972876e-001 -9.9956707e-001 -9.9976524e-001 -9.9990021e-001 -9.9981490e-001 -9.9970980e-001 -9.9959608e-001 -9.9985216e-001 -9.9982210e-001 -9.9939988e-001 -9.9976559e-001 -9.9995846e-001 -9.9994951e-001 -9.9983850e-001 -9.9981351e-001 -9.9878054e-001 -9.9857783e-001 -9.9961968e-001 -9.9998359e-001 -9.9982812e-001 -9.9868068e-001 -9.9984416e-001 -9.9992792e-001 -9.8657442e-001 -9.8176153e-001 -9.8951478e-001 -9.8503264e-001 -9.7388607e-001 -9.9403493e-001 -9.8653085e-001 -9.8361636e-001 -9.9235201e-001 -9.8049843e-001 -9.7227092e-001 -9.9494426e-001 -9.9756862e-001 -9.8408510e-001 -9.9433541e-001 -9.8527621e-001 -9.9986371e-001 -9.9966608e-001 -9.9993462e-001 -9.9034389e-001 -9.9483569e-001 -9.9441158e-001 -7.1240225e-001 -6.4484236e-001 -8.3899298e-001 -1.0000000e+000 -1.0000000e+000 -1.0000000e+000 -2.5754888e-001  9.7947109e-002  5.4715105e-001  3.7731121e-001  1.3409154e-001  2.7337197e-001 -9.1261831e-002 -4.8434650e-001 -7.8285070e-001 -9.9986502e-001 -9.9993178e-001 -9.9997295e-001 -9.9997018e-001 -9.9993012e-001 -9.9995862e-001 -9.9992899e-001 -9.9998465e-001 -9.9986326e-001 -9.9996815e-001 -9.9993610e-001 -9.9995363e-001 -9.9986442e-001 -9.9996098e-001 -9.9945373e-001 -9.9997811e-001 -9.9999153e-001 -9.9999010e-001 -9.9996857e-001 -9.9980657e-001 -9.9834600e-001 -9.9896122e-001 -9.9961874e-001 -9.9998934e-001 -9.9993540e-001 -9.9838752e-001 -9.9964264e-001 -9.9997266e-001 -9.9995535e-001 -9.9997630e-001 -9.9990583e-001 -9.9998550e-001 -9.9993717e-001 -9.9975115e-001 -9.9907227e-001 -9.9992754e-001 -9.9995158e-001 -9.9990585e-001 -9.9989269e-001 -9.9944433e-001 -9.9994099e-001 -9.9995861e-001 -9.5215466e-001 -9.5613397e-001 -9.4887014e-001 -9.7432057e-001 -9.2572179e-001 -9.5215466e-001 -9.9828520e-001 -9.7327320e-001 -6.4637645e-001 -7.9310345e-001 -8.8436120e-002 -4.3647104e-001 -7.9684048e-001 -9.9372565e-001 -9.9375495e-001 -9.9197570e-001 -9.9336472e-001 -9.8817543e-001 -9.9372565e-001 -9.9991844e-001 -9.9136366e-001 -1.0000000e+000 -9.3650794e-001  3.4698853e-001 -5.1608015e-001 -8.0276003e-001 -9.8013485e-001 -9.6130944e-001 -9.7365344e-001 -9.5226383e-001 -9.8949813e-001 -9.8013485e-001 -9.9924035e-001 -9.9265553e-001 -7.0129141e-001 -1.0000000e+000 -1.2898890e-001  5.8615643e-001  3.7460462e-001 -9.9199044e-001 -9.9069746e-001 -9.8994084e-001 -9.9244784e-001 -9.9104773e-001 -9.9199044e-001 -9.9993676e-001 -9.9045792e-001 -8.7130580e-001 -1.0000000e+000 -7.4323027e-002 -2.9867637e-001 -7.1030407e-001 -1.1275434e-001  3.0400372e-002 -4.6476139e-001 -1.8445884e-002 -8.4124676e-001  1.7994061e-001 -5.8626924e-002'"
     ]
    }
   ],
   "source": [
    "# 사용자 행동 데이터 세트 예측 분류\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행시간 측정을 취함. 시작 시간 설정\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간: {0:.1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로, GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다.\n",
    "\n",
    "그러나 수행시간이 오래 걸리고(GBM이 극복해야 할 중요한 과제), 하이퍼 파라미터 튜닝 노력도 더 필요하다.\n",
    "(반면에 랜덤 포레스트는 상대적으로 빠른 수행 시간을 보장해주기 떄문에 더 쉽게 예측 결과 도출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM 하이퍼 파라미터 튜닝\n",
    "- loss : 경사 하강법에서 사용할 비용함수 지정(default : deviance)\n",
    "\n",
    "\n",
    "- learning rage : GBMdㅣ 학습을 진행할 때마다 적용하는 학습률. weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수.\n",
    "    - 0~1 사이 값을 지정할 수 있으며, 기본값은 0.1이다.\n",
    "    - 너무 작은 값 적용할 시 : \n",
    "        - 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성 높다.\n",
    "        - 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다.\n",
    "    - 많은 weak learner :\n",
    "        - 순차적인 반복이 필요해 수행시간이 오래걸린다.\n",
    "        - 최소 오류 값을 찾지 못하고 그냥 지나쳐버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행 가능\n",
    "    - 따라서 이러한 특성때문에 n_estimator와 상호 보완적으로 조합해 사용한다.\n",
    "        - learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측성능이 조금씩 좋아질 수 있으나 시간이 오래걸리고 성능 역시 현격히 좋아지지는 않음\n",
    "\n",
    "\n",
    "- n_estimators : weak learner의 개수\n",
    "    - weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있지만, 개수가 많을 수록 시간 오래걸림\n",
    "    \n",
    "\n",
    "- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율\n",
    "    - 기본값 1, 이는 전체 학습 데이터를 기반으로 학습한다는 의미 \n",
    "    - 과적합이 염려되는 경우 1보다 작은 값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV를 이용해 하이퍼 파라미터를 최적화\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [100, 500],\n",
    "    'learning_rate' : [0.05, 0.1]\n",
    "}\n",
    "grid_cv = GridSearchCV(gb_clf, parma_gird=params, cv=2, verbose=1)\n",
    "gird_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 설정 그대로 테스트 데이터 세트에 적용해 에측 정확도 확인\n",
    "\n",
    "# GridSearchCV를 이용해 최적으로 학습된 estimator로 예측 수행\n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. XGBoost(eXtra Gradient Boost)\n",
    ": 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나\n",
    "- GBM에 기반하고 있지만, GBM의 단점인 느린 수행시간 및 과적합 규제 부재 등의 문제를 해결했다.\n",
    "    - 병렬 CPU 환경에서 병렬학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다.\n",
    "    \n",
    "    \n",
    "- **XGBoost 장점**\n",
    "    - 뛰어난 예측 성능 발휘\n",
    "    - GBM 대비 빠른 수행 시간\n",
    "        - XBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 가지고 있다. \n",
    "        - GBM에 비해 수행시간이 빠르다는 것이지 다른 머신러닝 알고리즘에 비해 빠르다는 의미는 아님.\n",
    "    - 과적합 규제\n",
    "        - XGBoost는 자체에 과적합 규제 기능으로 좀 더 강한 내구성을 가진다.\n",
    "    - Tree pruning(나무 가지치기)\n",
    "        - GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조정하기도 하지만, tree pruning으로 더이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.\n",
    "    - 자체 내장된 교차 검증\n",
    "       - 반복 수행시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차검증을 수행해 최적화된 바복 수행 횟수를 가질 수 있다.\n",
    "       - 지정된 반복 횟수가 아닌 교차검증을 통해 평가 데이터 세트의 평가 값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단기능이 있다.\n",
    "           - 조기 중단기능은 수행 속도를 향상시킨다.\n",
    "           - n_estimators에 지정한 부스팅 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선한다.\n",
    "    - 결손값 자체 처리 기능\n",
    "    - 자체적으로 성능 평가, 피처 중요도 등의 시각화 기능을 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost의 파이썬 패키지명 : xgboost\n",
    "    - 초기 출시에는 사이킷런과 호환되지 않는 독자적인 XgBoost 전용의 패키지였다.\n",
    "    - 사람들이 사이킷런을 많이 사용하기 때문데 사이킷런과 연동할 수 있는 래퍼 클래스(Wrapper class)를 제공했다.\n",
    "- XGBoost 패키지의 사이킷런 래퍼 클래스 : XGBClassifier, XGBRegressor\n",
    "    - 이를 이용하면 사이킷런 estimator가 학습을 위해 사용하는 fit과 predict과 같은 표준 사이킷런 개발 프로세스 및 다양한 유틸리티 활용 가능\n",
    "- 사이킷런 래퍼 XGBoost 모듈은 사이킷런의 다른 estimator와 사용법이 같은데, 파이썬 네이티브 XGBoost는 고유의 API 하이퍼 파라미터를 이용한다. 크게 다르지는 않으나 몇 가지 주의할 점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 하이퍼 파라미터\n",
    "- 파이썬 래퍼 XGBoost 모듈과 사이킷런 래퍼 XGBoost 모듈의 일부 하이퍼 파라미터는 약간 다르므로 주의가 필요하다.\n",
    "    - 동일한 기능을 의미하는 하이퍼 파라미터지만, 사이킷런 파라미터의 범용화된 이름 규칙에 따라 파라미터 명이 달라짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 일반 파라미터 >\n",
    ": 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터.(디폴트 파라미터 값을 바꾸는 경우는 거의 없다.)\n",
    "- **booster**\n",
    "    - gbtree(tree based model)또는 gblinear(linear model) 선택. \n",
    "        - 디폴트는 gbree\n",
    "- **silent**\n",
    "    - 디폴트는 0, 출력 메시지를 나타내고 싶지 않을 경우 1\n",
    "- **nthread** \n",
    "    - CPU의 실행 스레드 개수 조정\n",
    "        - 디폴트는 CPU의 전체 스레드를 다 사용하는 것.\n",
    "    - 멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해서 ML 애플리케이션을 구동하는 경우에 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 주요 부스터 파라미터 >\n",
    ": 트리 최적화, 부스팅, regularization 등과 관련 파라미터 등을 지칭한다.\n",
    "- **eta[default=0.3, alias:learning_rate]**\n",
    "    - GBM의 학습률(learning_rate)와 같은 파라미터.\n",
    "    - 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값.\n",
    "        - 사이킷런 래퍼 클래스를 이용할경우 eta는 learning_rate 파라미터로 대체되며, 디폴트는 0.1이다. 보통은 0.01 ~ 0.2 사이의 값을 선호한다.\n",
    "        \n",
    "        \n",
    "- **num_boost_rounds**\n",
    "    - GBM의 n_estimator과 같은 파라미터이다.\n",
    "    \n",
    "    \n",
    "- **min_child_weight[default=1]**\n",
    "    - 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합\n",
    "    - 클수록 분할을 자제한다.\n",
    "    - 과적합을 조절하기 위해 사용\n",
    "    \n",
    "    \n",
    "- **gamma[default=0, alias:min_split_loss]**\n",
    "    - 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값.\n",
    "    - 새당 값보다 큰 손실(loss)이 감소된 경우에 리프 노드를 분리한다.\n",
    "    - 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **max_depth[default=6]**\n",
    "    - 트리 기반 알고리즘의 max_depth와 같다.\n",
    "    - 0을 지정하면 깊이 제한 X. 보통은 3~10 사이의 값을 적용한다.\n",
    "    - depth가 높으면 과적합 가능성이 높아진다.\n",
    "    \n",
    "    \n",
    "- **sub_sample[default=1]**\n",
    "    - GBM의 subsample과 동일. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정ㅎ나다.\n",
    "    - 0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는데 사용한다. \n",
    "    - 일반적으로 0.5 ~ 1 사이의 값을 사용한다.\n",
    "    \n",
    "    \n",
    "- **colsample_bytree[default=1]**\n",
    "    - GBM의 max_features와 유사. 트리 생성에 필요한 피처를 임의로 샘플링하는데 사용\n",
    "    - 매우 많은 피처가 있는 경우 과적합을 조정하는데 적용\n",
    "    \n",
    "    \n",
    "- **lambda[default=1, alias:reg_lambda]**\n",
    "    - L2 Regularization 적용 값. 피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **alpha[default=0, alias:reg_alpha]**\n",
    "    - L1 Regularization 적용 값.피처 개수가 많은 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.\n",
    "    \n",
    "    \n",
    "- **scale_pos_wight[default=1]**\n",
    "    - 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 학습 태스크 파라미터 >\n",
    "- **objective**\n",
    "    - 최솟값을 가져야 할 손실 수 정의. \n",
    "    - 주로 사용되는 손실 함수는 이진 분류인지 다중 분류인지에 따라 달라진다.\n",
    "    \n",
    "    \n",
    "- **binary:logistic**\n",
    "    - 이진 분류일 때 적용\n",
    "    \n",
    "- **multi:softmax**\n",
    "    - 다중 분류일 때 적용.\n",
    "    - 손실함수가 multi:softmax일 경우에는 레이블 클래스 개수인 num_class 파라미터를 지정해야 한다.\n",
    "\n",
    "\n",
    "- **multi:softpob**\n",
    "    - multi:softmax와 유사하나, 개별 레이블 클래스의 해당되는 예측 확률을 반환한다.\n",
    "\n",
    "\n",
    "- **eval_metric**\n",
    "    - 검증에 사용되는 함수를 정의.\n",
    "    - 기본값은 회귀인 경우는 rmse, 분류일 경우에는 error이다.\n",
    "    - **eval_metirc의 값 유형**\n",
    "        - **rmse** : 평균 제곱근 오차(Root Mean Square Error)\n",
    "            - 잔차의 제곱합을 산술평균한 값의 제곱근. 관측값들간의 상호간 편차\n",
    "        - **mae** : 평균 절대 오차(Mean Absolute Error)\n",
    "            - 오차의 절대값의 평균\n",
    "        - **logloss** : 음의 로그 가능도(Negative log-likelihood)\n",
    "            - 로그가능도 x -1. 확률 변수가 독립확률변수로 나누어지는 경우와 같이 확률분포함수가 곱셈꼴로 나올때 미분계산의 편의성을 위해 사용한다.\n",
    "        - **error** : 이진분류 오류율(Binary classification error rate(0.5 threshold))\n",
    "        - **merror** : 다중 클래스 분류 오류율(Multicalss classification error rate)\n",
    "        - **mlogloss** : 다중클래스의 로그 손실(Multiclass logloss)\n",
    "        - **acu** : 곡선 하위 영역(Area under the curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뛰어난 알고리즘일수록 파라미터를 튜닝할 필요가 적다. 또한 파라미터 튜닝에 들이는 공수 대비 성능 향상 효과가 높지 않은 경우가 대부분이다.\n",
    "\n",
    "파라미터를 튜닝하는 경우의 수는 여러 가지 상황에 따라 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과적합 문제가 심각할 경우 다음과 같이 적용해본다.\n",
    "    - eta값을 낮춘다.(0.01 ~ 0.1) eta값을 낮출 경ㅇ우 num_round(또는 n_estimators)는 반대로 높여줘야함\n",
    "    - max_depth 값을 낮춘다.\n",
    "    - min_child_weight 값을 높인다.\n",
    "    - gamma 값을 높인다.\n",
    "    - subsample과 colsample_bytree를 조정하는 것도 트리가 너무 복잡하게 생성되는 것을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 버전 확인\n",
    "import xgboost \n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 래퍼 XGBoost 적용 - 위스콘신 유방암 예측\n",
    "위스콘신 유방암 데이터 세트를 활용하여 파이썬 래퍼 XGBoost API의 사용법을 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 및 데이터 로드\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance # 피처의 중요도 시각화\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wanings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_features = dataset.data\n",
    "y_label = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\n",
    "cancer_df['target'] = y_label\n",
    "cancer_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "종양의 크기와 모양에 관련된 많은 속성이 숫자형 값으로 되어 있다.\n",
    "\n",
    "타깃 레이블 값의 종류는 악성인 malignant가 0, 양성인 benign이 1값으로 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 값의 분포 확인\n",
    "\n",
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "print(x_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬 래퍼 XGBoost와 사이킷런의 차이\n",
    "먼저 눈에 띄는 차이는 파이썬 래퍼 XGBoost는 학습용과 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다.\n",
    "- DMatrix : 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트\n",
    "    - 주요 파라미터\n",
    "        - data : 피처 데이터 세트\n",
    "        - label : 분류의 경우에는 레이블 데이터세트, 회귀의 경우는 숫자형인 종솟값 데이터 세트\n",
    "    - DMatrix는 넘파이 이외에 libsvm txt 포맷 파일, xgboost 이진 버퍼 파일을 파라미터로 입력받아 변환할 수 있다. \n",
    "        - 판다스의 데이터프레임으로 데이터 인터페이스를 하기 위해서는 DataFrame.values를 이용해 넘파이로 일차 변환한 뒤에 이를 이용해 DMatrix 변환을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 형태의 학습 데이터 세트와 테스트 데이터 세트를 DMatrix로 변환\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.Dmatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 래퍼 XGBoost 모듈인 xgboost를 이용해 학습을 수행하기 전에, 먼저 XGBoost의 하이퍼 파라미터 설정. 주로 !!딕셔너리 형태!!로 입력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'max_depth':3,\n",
    "          'eta':0.1,\n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric':'logloss',\n",
    "          'earl_stoppings':100 # 조기 중단할 수 있는 최소 반복 횟수\n",
    "}\n",
    "num_rounds = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 하이퍼 파라미터를 xgboost 모듈의 train()함수에 파라미터로 전달한다.(사이킷런의 경우는 Estimator의 생성자를 하이퍼 파라미터로 전달)\n",
    "\n",
    "\n",
    "- **early_stopping_rounds 파라미터 입력**을 통해 **조기중단**할 수 있도록 설정한다.\n",
    "    - early_stopping_rounds 파라미터를 설정해 조기 중단을 수행하기 위해서는 반드시 **eval_set**과 **eval_metric**이 함께 설정되어야 한다.\n",
    "        - XGBoost는 반복마다 eval_set으로 지정된 데이터 세트에서 eval_metric의 지정된 평가 지표로 예측 오류를 측정한다.\n",
    "        - **eval_set** : 성능 평가를 수행할 평가용 데이터 세트 설정\n",
    "        - **eval_metric** : 평가 세트에 적용할 성능 평가 방법. 분류일 경우 주로 error, logloss를 적용\n",
    "    - xgboost train()의 **evals 파라미터**에 학습된 데이터 세트와 eval 데이터 세트를 명기해주면, 평가를 eval 데이터 세트에 수행하면서 조기 중단을 적용할 수 있다. 조기 중단을 수행하려면 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명기\n",
    "wlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train()함수의 파라미터로 전달\n",
    "# 반복시마다 evals에 표시된 데이터 세트에 대해 평가지표 결과 출력된다.\n",
    "# train()은 학습이 완료된 모델 객체를 반환\n",
    "xgb_model = xgb.train(params = paras, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=100, evals=wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬 래퍼 XGBoost는 train()함수를 호출해 학습이 완료된 모델 객체를 반환하게 되는데, 이 모델 객체는 예측을 위해 predict() 메서드를 이용한다.\n",
    "    - 유의할 점은 사이킷런의 predict()메서드는 예측 결과 클래스 값(즉 0, 1)을 반환하는데 반해 xgboost의 predict()는 예측 결괏값이 아닌 예측 결과를 추정할 수 있는 확률 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict() 수행 결괏값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "# 예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정해 리스트 객체인 preds에 저장\n",
    "preds = [1 if x > 0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시: ', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 모델의 예측 성능 평가\n",
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xboost의 plot_importance() : 피처의 중요도를 막대그래프 형식으로 나타냄\n",
    "    - 기본 평가 지표로 f1스코어를 기반으로 한다.\n",
    "    - 호출 시 파라미터로 앞에서 학습이 완료된 앞델 객체 및 맷플롯립의 ax 객체를 입력하기만 하면 된다.\n",
    "    - (사이킷런은 estimator 객체의 feature_importances_속성을 이용해 직접 시각화 코드를 작성해야 한다.)\n",
    "    - 유의 할 점\n",
    "        - xgboost 넘파이 기반의 피처 데이터로 학습 시에 피처명을 제대로 알 수가 없으므로 f0, f1와 같이 피처 순서별로 f자 뒤에 숫자를 붙여서 X축에 피처들로 나열한다. (f0이 첫번째 피처)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "plot_importance(xgb_model, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xgboost 모듈의 to_graphviz() API : 이를 이용하면 주피터 노트북에 바로 규칙 트리 구조를 그릴 수 있다.\n",
    "    - Graphviz 프로그램과 패키지가 설치돼 있어야 한다.\n",
    "    - xgboost.to_graphviz()내에 파라미터로 학습이 완료된 모델 객체와 Graphviz가 참조할 파일명을 입력해주면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv( ) : 파이썬 래퍼 XGBoost에서 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 제공\n",
    "    - **파라미터**\n",
    "        - **params** (dict) : 부스터 파라미터\n",
    "        - **dtrain** (DMatrix) : 학습 데이터\n",
    "        - **num_boost_round** (int) : 부스팅 반복 횟수\n",
    "        - **nfold** (int) : CV 폴드 개수\n",
    "        - **stratified** (bool) : CV 수행 시 층화 표본 추출(stratified sampling) 수행 여부\n",
    "        - **metrics** (string or list of string) : CV 수행 시 모니터링할 성능 평가 지표\n",
    "        - **early_stopping_rounds** (int) : 조기 중단을 활성화시킴. 반복 횟수 지정\n",
    "    \n",
    "    \n",
    "    - 데이터프레임 형태로 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 래퍼 XGBoost의 개요 및 적용\n",
    "- 다른 estimator와 동일하게 fit과 predict만으로 학습과 예측이 가능하고, GridSearchCV, Pipeline등 사이킷런의 다른 유틸리티를 그대로 사용할 수 있다.\n",
    "    - 따라서 기존의 다른 머신러닝 알고리즘으로 만들어놓은 프로그램이 있더라도 알고리즘 클래스만 XGBoost 래퍼 클래스로 바꾸면 기존 클래스 그대로 사용 가능\n",
    "    \n",
    "    \n",
    "- XGBClassifier(분류를 위한 래퍼 클래스), XGBRegressor(회귀를 위한 래퍼 클래스)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBClassifier는 기존 사이킷런에서 일반적으로 사용하는 하이퍼 파라미터와 호환성을 유지하기 위해 파이썬 래퍼 xgboost 모듈에서 사용하던 네이티브 하이퍼 파라미터 몇 개를 다음과 같이 변경했다.\n",
    "    - eta -> learning_rate\n",
    "    - sub_sample -> subsample\n",
    "    - lambda -> reg_lambda\n",
    "    - alpha -> reg_alpha\n",
    "    \n",
    "\n",
    "- xgboost의 n_estimator와 num_boost_round 하이퍼 파라미터는 서로 동일\n",
    "    - 만일 두개가 동시에 사용 되면\n",
    "        - 파이썬 래퍼 XGBoost는 num_boost_round 파라미터 적용\n",
    "        - XGBClassifier와 같은 사이킷런 래퍼 클래스에서는 n_estimator 파라미터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위스콘신 대학병원의 유방암 데이터 세트를 분류를 위한 래퍼 클래스인 XGBClassifier를 이용해 예측\n",
    "\n",
    "# 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb_wrapper.fit(X_train, y_trian)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba = xgb_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 성능 평가\n",
    "\n",
    "get_clf_eval(y_test, w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런 래퍼 XGBoost에서의 조기 중단 수행\n",
    "    - 조기 중단 관련한 파라미터를 fit()에 입력한다.\n",
    "        - early_stopping_rounds : 평가 지표가 향상될 수 있는 반복 횟수 정의\n",
    "        - eval_metic : 조기 중단을 위한 평가 지표\n",
    "        - eval_set : 성능 평가를 수행할 데이터 세트\n",
    "            - (학습데이터가 아니라 별도의 데이터 세트여야한다. 학습시에는 완전히 알려지지 않은 데이터 세트 사용해야함. 왜냐면 학습 시에 미리 참고가 되어 과적합할 수 있기 때문)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 예제에서는 데이터 세트의 크기가 작아 테스트 데이터를 평가용으로 사용했다. 이부분을 주지\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_esimators=400, learning_rate=0.1, max_depth=3)\n",
    "evals = [(X_test, y_test)]\n",
    "xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "# verbose=True : 부팅시 상세정보를 보여줌으로써, 어느 부분에서 어떤 지연이 있는지 확인 가능\n",
    "ws100_preds = xgb_wrapper.predict(X_test)\n",
    "ws100_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "211번 반복시 logloss가 0.085593이고 311번 반복 시 logloss가 0.085948인데, 211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에 더이상 반복하지 않고 멈추었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기 중단으로 학습된 XGBClassifier의 예측성능\n",
    "get_clf_eval(y_test, ws100_preds, ws100_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기 중단을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. \n",
    "\n",
    "아직 성능이 향상될 여지가 있음에도 불구하고 n번 반복하는 동안 성능 평가 지표가 향상되지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측성능이 나빠질 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping_rounds를 10으로 설정하고 재학습\n",
    "xgb_wrapper.fit(X_train, y_train, earl_stopping_rounds=10, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "ws10_preds = xgb_wrapper.predict(X_test)\n",
    "ws10_preds_proba = wgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, ws10_preds, ws10_preds_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "early_stopping_rounds=100일때보다 정확도가 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 피처의 중요도를 시각화하는 모듈인 plot_importance()에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 도출해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "# 사이킷런 wrapper 클래스를 입력해도 무방\n",
    "plot_importance(xgb_wrapper, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. 분류 실습 - 캐글 산탄데르 고객 만족 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (76020, 371)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "2   4     2     23                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "0                      0.0                      0.0  ...   \n",
       "1                      0.0                      0.0  ...   \n",
       "2                      0.0                      0.0  ...   \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "2                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
       "0                     0.0                     0.0  39205.17       0  \n",
       "1                     0.0                     0.0  49278.03       0  \n",
       "2                     0.0                     0.0  67333.77       0  \n",
       "\n",
       "[3 rows x 371 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "cust_df = pd.read_csv(\"/Users/wizdom/Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/santader_customer_satisfaction/train_santader.csv\",\n",
    "                     encoding='latin-1')\n",
    "print('dataset shape:', cust_df.shape)\n",
    "cust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76020 entries, 0 to 76019\n",
      "Columns: 371 entries, ID to TARGET\n",
      "dtypes: float64(111), int64(260)\n",
      "memory usage: 215.2 MB\n"
     ]
    }
   ],
   "source": [
    "cust_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    73012\n",
      "1     3008\n",
      "Name: TARGET, dtype: int64\n",
      "Unsatisfied 비율은 0.04\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터에서 만족과 불만족의 비율 살펴보기\n",
    "\n",
    "print(cust_df['TARGET'].value_counts())\n",
    "unsatisfied_cnt = cust_df[cust_df['TARGET']==1].TARGET.count()\n",
    "total_cnt = cust_df.TARGET.count()\n",
    "print('Unsatisfied 비율은 {0:.2f}'.format(unsatisfied_cnt / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>76020.000000</td>\n",
       "      <td>7.602000e+04</td>\n",
       "      <td>76020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75964.050723</td>\n",
       "      <td>-1523.199277</td>\n",
       "      <td>33.212865</td>\n",
       "      <td>86.208265</td>\n",
       "      <td>72.363067</td>\n",
       "      <td>119.529632</td>\n",
       "      <td>3.559130</td>\n",
       "      <td>6.472698</td>\n",
       "      <td>0.412946</td>\n",
       "      <td>0.567352</td>\n",
       "      <td>...</td>\n",
       "      <td>7.935824</td>\n",
       "      <td>1.365146</td>\n",
       "      <td>12.215580</td>\n",
       "      <td>8.784074</td>\n",
       "      <td>31.505324</td>\n",
       "      <td>1.858575</td>\n",
       "      <td>76.026165</td>\n",
       "      <td>56.614351</td>\n",
       "      <td>1.172358e+05</td>\n",
       "      <td>0.039569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43781.947379</td>\n",
       "      <td>39033.462364</td>\n",
       "      <td>12.956486</td>\n",
       "      <td>1614.757313</td>\n",
       "      <td>339.315831</td>\n",
       "      <td>546.266294</td>\n",
       "      <td>93.155749</td>\n",
       "      <td>153.737066</td>\n",
       "      <td>30.604864</td>\n",
       "      <td>36.513513</td>\n",
       "      <td>...</td>\n",
       "      <td>455.887218</td>\n",
       "      <td>113.959637</td>\n",
       "      <td>783.207399</td>\n",
       "      <td>538.439211</td>\n",
       "      <td>2013.125393</td>\n",
       "      <td>147.786584</td>\n",
       "      <td>4040.337842</td>\n",
       "      <td>2852.579397</td>\n",
       "      <td>1.826646e+05</td>\n",
       "      <td>0.194945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.163750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38104.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.787061e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76043.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.064092e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113748.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187563e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151838.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>210000.000000</td>\n",
       "      <td>12888.030000</td>\n",
       "      <td>21024.810000</td>\n",
       "      <td>8237.820000</td>\n",
       "      <td>11073.570000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50003.880000</td>\n",
       "      <td>20385.720000</td>\n",
       "      <td>138831.630000</td>\n",
       "      <td>91778.730000</td>\n",
       "      <td>438329.220000</td>\n",
       "      <td>24650.010000</td>\n",
       "      <td>681462.900000</td>\n",
       "      <td>397884.300000</td>\n",
       "      <td>2.203474e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID           var3         var15  imp_ent_var16_ult1  \\\n",
       "count   76020.000000   76020.000000  76020.000000        76020.000000   \n",
       "mean    75964.050723   -1523.199277     33.212865           86.208265   \n",
       "std     43781.947379   39033.462364     12.956486         1614.757313   \n",
       "min         1.000000 -999999.000000      5.000000            0.000000   \n",
       "25%     38104.750000       2.000000     23.000000            0.000000   \n",
       "50%     76043.000000       2.000000     28.000000            0.000000   \n",
       "75%    113748.750000       2.000000     40.000000            0.000000   \n",
       "max    151838.000000     238.000000    105.000000       210000.000000   \n",
       "\n",
       "       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 72.363067               119.529632   \n",
       "std                 339.315831               546.266294   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               12888.030000             21024.810000   \n",
       "\n",
       "       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  3.559130                 6.472698   \n",
       "std                  93.155749               153.737066   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max                8237.820000             11073.570000   \n",
       "\n",
       "       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "count             76020.000000             76020.000000  ...   \n",
       "mean                  0.412946                 0.567352  ...   \n",
       "std                  30.604864                36.513513  ...   \n",
       "min                   0.000000                 0.000000  ...   \n",
       "25%                   0.000000                 0.000000  ...   \n",
       "50%                   0.000000                 0.000000  ...   \n",
       "75%                   0.000000                 0.000000  ...   \n",
       "max                6600.000000              6600.000000  ...   \n",
       "\n",
       "       saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                  7.935824                 1.365146   \n",
       "std                 455.887218               113.959637   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max               50003.880000             20385.720000   \n",
       "\n",
       "       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\n",
       "count            76020.000000            76020.000000   \n",
       "mean                12.215580                8.784074   \n",
       "std                783.207399              538.439211   \n",
       "min                  0.000000                0.000000   \n",
       "25%                  0.000000                0.000000   \n",
       "50%                  0.000000                0.000000   \n",
       "75%                  0.000000                0.000000   \n",
       "max             138831.630000            91778.730000   \n",
       "\n",
       "       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "count             76020.000000             76020.000000   \n",
       "mean                 31.505324                 1.858575   \n",
       "std                2013.125393               147.786584   \n",
       "min                   0.000000                 0.000000   \n",
       "25%                   0.000000                 0.000000   \n",
       "50%                   0.000000                 0.000000   \n",
       "75%                   0.000000                 0.000000   \n",
       "max              438329.220000             24650.010000   \n",
       "\n",
       "       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\n",
       "count            76020.000000            76020.000000  7.602000e+04   \n",
       "mean                76.026165               56.614351  1.172358e+05   \n",
       "std               4040.337842             2852.579397  1.826646e+05   \n",
       "min                  0.000000                0.000000  5.163750e+03   \n",
       "25%                  0.000000                0.000000  6.787061e+04   \n",
       "50%                  0.000000                0.000000  1.064092e+05   \n",
       "75%                  0.000000                0.000000  1.187563e+05   \n",
       "max             681462.900000           397884.300000  2.203474e+07   \n",
       "\n",
       "             TARGET  \n",
       "count  76020.000000  \n",
       "mean       0.039569  \n",
       "std        0.194945  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 371 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 피처의 값 분포 간단히 확인\n",
    "\n",
    "cust_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var3 칼럼의 경우 min값이 -999999이다. NaN이나 특정 예외 값을 -999999로 변환했을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2         74165\n",
      " 8           138\n",
      "-999999      116\n",
      " 9           110\n",
      " 3           108\n",
      " 1           105\n",
      " 13           98\n",
      " 7            97\n",
      " 4            86\n",
      " 12           85\n",
      "Name: var3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cust_df.var3.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-999999값이 116개나 있다. var3는 숫자형이고 -999999는 다른 값에 비해 너무 편차가 심하므로, 이를 가장 값이 많은 2로 변환하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피처 데이터 shape:(76020, 369)\n"
     ]
    }
   ],
   "source": [
    "cust_df['var3'].replace(-999999, 2, inplace=True)\n",
    "# ID 피처는 단순 식별자에 불과하므로 드롭해주기\n",
    "cust_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# 피처 세트와 레이블 세트 분리. 레이블 칼럼은 데이터프레임의 맨 마지막에 위치하므로 칼럼 위치를 -1로 하여 분리\n",
    "X_features = cust_df.iloc[:, :-1]\n",
    "y_labels = cust_df.iloc[:, -1]\n",
    "print('피처 데이터 shape:{0}'.format(X_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369)\n",
      "\n",
      "학습 세트 레이블 값 분포 비율\n",
      " 0    0.960964\n",
      "1    0.039036\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "테스트 세트 레이블 값 분포 비율\n",
      " 0    0.9583\n",
      "1    0.0417\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 학습과 성능 평가를 위해 학습 데이터 세트와 테스트 데이터 세트 분리\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# 비대칭한 데이터 세트이므로 클래스인 Target 값 분포도가 학습 데이터와 테스트 데이터 세트에 모두 비슷하게 추출됐는지 확인\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape, X_test.shape))\n",
    "\n",
    "print('\\n학습 세트 레이블 값 분포 비율\\n', y_train.value_counts()/train_cnt)\n",
    "print('\\n테스트 세트 레이블 값 분포 비율\\n', y_test.value_counts()/test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습과 테스트 데이터 세트 모두 TARGET의 값의 분포가 원본 데이터와 유사하게 전체 데이터의 4%정도의 불만족 값으로 만들어졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정\n",
    "xgb_clf = XGBClassifier(n_estimators=500, random_state=156)\n",
    "\n",
    "# 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", \n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터 세트를 XGBoost 평가 데이터 세트로 사용하면 과적합의 가능성을 증가시킬 수 있지만, 여기서는 이러한 점만 주지하고 넘어가자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f22961484a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 하이퍼 파라미터 테스트의 수행 속도를 향상 시키기 위해 n_estimators를 100으로 감소(나중에 하이퍼 파라미터 튜닝이 완료되면 다시 증가시킨다.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 칼럼의 개수가 많으므로 과적합 가능성을 가정하고, max_depth, min_child_weight, colsample_bytree 하이퍼 파라미터만 일차 튜닝 대상으로 한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# XGBoost 하이퍼 파라미터 튜닝 수행\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 하이퍼 파라미터 테스트의 수행 속도를 향상 시키기 위해 n_estimators를 100으로 감소(나중에 하이퍼 파라미터 튜닝이 완료되면 다시 증가시킨다.)\n",
    "xgb_clf = XGBClassifier(n_estimators=100)\n",
    "\n",
    "# 칼럼의 개수가 많으므로 과적합 가능성을 가정하고, max_depth, min_child_weight, colsample_bytree 하이퍼 파라미터만 일차 튜닝 대상으로 한다.\n",
    "params = {'max_depth':[5, 7], 'min_child_weight':[1, 3], 'colsample_bytree':[0.5, 0.75]}\n",
    "\n",
    "# cv는 3으로 지정\n",
    "gridcv = GridSearchCV(xgb_clf, param_grid=params, cv=3)\n",
    "# 수행시간이 오래 걸리므로 early_stopping_rounds도 30으로 줄인다.\n",
    "girdcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\", \n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "print('GridSearchCV 최적 파라미터: ', girdcv.best_params_)\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8448로 조금 개선되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 시간이 많이 필요한 ML 모델인 경우, 하이퍼 파라미터 튜닝을 수행하는 요령 중 첫 번째는 먼저 2 ~ 3개 정도의 파라미터를 결합해 최적 하라미터를 찾아낸 뒤에, 이 최적 파라미터를 기반으로 다시 1~2개 파라미터를 결합해 파라미터 튜닝을 수행하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 구한 최적화 하이퍼 파라미터를 기반으로 다른 하이퍼 파라미터를 변경 또는 추가해 다시 최적화 진행\n",
    "\n",
    "# n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함.\n",
    "xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7,\n",
    "                       min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03)\n",
    "\n",
    "# 성능 평가 지표를 auc로, 조기 중단 파라미터 값은 200으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=\"auc\",\n",
    "           eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8456으로 이전 테스트보다 살짝 향상된 결과를 나타내고 있다.\n",
    "- 한가지 아쉬운 점은 XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행 시간이 상당히 더 많이 요구된다는 점이다.\n",
    "    - 그 때문에 하이퍼 파라미터를 다양하게 나열해 파라미터를 튜닝하는 것은 많은 시간이 소모된다.\n",
    "- 앙상블 계열 알고리즘에서 하이퍼 파라미터 튜닝으로 성능 수치 개선이 급격하게 되는 경우는 많지 않다. \n",
    "    - 앙상블 계열 알고리즘은 과적합이나 잡음에 기본적으로 뛰어난 알고리즘이때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝된 모델에서 각 피처의 중요도를 그래프로 나타내기\n",
    "\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞의 XGBoost 예제 코드에서 만들어진 데이터 세트를 기반으로 학습 수행하고, ROC-AUC 측정\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=True)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV로 좀 더 다양한 하이퍼 파라미터에 대한 튜닝 수행\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 200으로 감소\n",
    "lgbm_clf = LGBMClassifier(n_estimators=200)\n",
    "\n",
    "pramas = {'num_leaves':[32, 64], 'max_depth':[128, 160], 'min_child_samples':[60,100], 'subsample':[0.8, 1]}\n",
    "\n",
    "# cv는 3으로 지정\n",
    "gridcv = GridSearchCV(lgbm_clf, param_grid=params, cv=3)\n",
    "gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\",\n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "print('GridSearchCV 최적 파라미터: ', girdcv.best_params_)\n",
    "lgbm_roc_score = roc_auc_score(y_test, girdcv.predict_proba(X_test)[:, 1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 파라미터를 적용하고 다시 학습해 ROC-AUC 측정 결과 도출\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100, max_depth=128)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=True)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average='macro')\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. 분류 실습 - 캐글 신용카드 사기 검출\n",
    "### 언더 샘플링과 오버 샘플링의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 예측 성능의 문제가 발생할 수 있는데, 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생한다. \n",
    "\n",
    "즉, 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면에 정상 레이블을 가지는 데이터 거누는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉽다.\n",
    "- 지도학습에서의 극도로 불균형한 레이블 값 분포로 인한 문제점 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요\n",
    "    - 오버 샘플링(Oversampling) 방법 \n",
    "        - 이상 데이터와 같이 적은 데이터를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법. \n",
    "        - 동일한 데이터를 단순히 증식하는 방법은 과적합이 되기 때문에 의미가 없으므로 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식한다.\n",
    "        - 언더 샘플링보다 오버 샘플링 방식이 예측 성능상 더 유리한 경우가 많아 주로 사용된다.\n",
    "        - 대표적으로 SMOTE(Synthetic Minority Over-sampling Technique) 방법이 있다. 적은 데이터 세트에 있는 개별 데이터들의 K 최근접 이웃(K Nearest Neighbor)을 찾아서 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식\n",
    "        - SMOTE를 구현한 대표적인 파이썬 패키지는 imbalanced-learn\n",
    "    \n",
    "    \n",
    "     - 언더 샘플링(Undersampling) 방법\n",
    "        - 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식\n",
    "        - 정상 레이블 데이터를 이상 레이블 데이터 수준으로 줄여버린 상태에서 학습을 수행하면 과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기 때문에 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용하지 않는 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 일차 가공 및 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "card_df = pd.read_csv('/Users/wizdom/Desktop/data_analysis/파이썬 머신러닝 완벽가이드/실습 데이터/creditcard.csv')\n",
    "card_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time 피처는 데이터 생선 관련한 작업용 속성이므로 큰 의미가 없으므로 제거. Amount 피처는 신용카드 트랙잭션 금액을 의미하며, Class는 레이블로서 0의 경우 정상, 1의 경우 사기 트랜잭션이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "card_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 인자로 입력받은 Dataframe을 복사한 뒤 Time 칼럼만 삭제하고 복사된 dataframe 반환\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop('Time', axis = 1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 데이터 가공 후, 학습/테스트 데이터 세트 반환하는 함수\n",
    "def get_train_test_dataset(df=None):\n",
    "    # 인자로 입력된 dataframe의 사전 데이터 가공이 완료된 복사 dataframe 반환\n",
    "    df_copy = get_preprocessed_df(df)\n",
    "    # dataframe의 맨 마지막 칼럼이 레이블, 나머지는 피처들\n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    # train_test_split()으로 학습과 테스트 데이터 분할.\n",
    "    # stratify=y_target으로 Stratified기반 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3,\n",
    "                                                       random_state=0, stratify=y_target)\n",
    "    # 학습과 테스트 데이터 세트 반환\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 레이블 값 비율\n",
      "0    99.827451\n",
      "1     0.172549\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "테스트 데이터 레이블 값 비율\n",
      "0    99.826785\n",
      "1     0.173215\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 세트와 테스트 데이터 세트 레이블 값 비율 확인\n",
    "print('학습 데이터 레이블 값 비율')\n",
    "print(y_train.value_counts() / y_train.shape[0] * 100)\n",
    "print('\\n테스트 데이터 레이블 값 비율')\n",
    "print(y_test.value_counts() / y_test.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 레이블과 테스트 데이터 레이블이 큰 차이 없이 잘 분할되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀를 이용한 신용 카드 사기 여부 예측\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차행렬\n",
      " [[85279    16]\n",
      " [   59    89]]\n",
      "정확도: 0.9991, 정밀도: 0.8476, 재현율: 0.6014, F1: 0.7036, AUC: 0.9547\n"
     ]
    }
   ],
   "source": [
    "# 3장에서 사용한 get_clf_eval() 함수를 이용해 평가 수행\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "\n",
    "def get_clf_eval(y_test, pred, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차행렬\\n', confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))  \n",
    "    \n",
    "get_clf_eval(y_test, lr_pred, lr_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자로 사이킷런의 Estimator객체와 학습/테스트 데이터 세트를 입력 받아서 학습/예측/평가 수행\n",
    "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "    model.fit(ftr_train, tgt_train)\n",
    "    pred = model.predict(ftr_test)\n",
    "    pred_proba = model.predict_proba(ftr_test)[:, 1]\n",
    "    get_clf_eval(tgt_test, pred, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2990347be26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LightGBM을 이용한 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 본 데이터 세트는 극도로 불균형한 레이블 값 분포도를 가지고 있으므로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# LightGBM을 이용한 모델\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 본 데이터 세트는 극도로 불균형한 레이블 값 분포도를 가지고 있으므로 \n",
    "# LGBMClassifier 객체 생성 시 boost_from_average=False로 파라미터 설정해야 한다.\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀 : 재현율 0.6014, ROC-AUC 0.9547\n",
    "- LightGBM : 재현율 0.7568, ROC-AUC 0.9797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------- 데이터 재가공 뒤에 모델 다시 테스트 ----------------------\n",
    "### 1. 데이터 분포도 변환 후 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀는 선형 모델이다. 대부분의 선형 모델은 중요 피처들의 값이 정규 분포 형태를 유지하는 것을 선호한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'xtricks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dcdddd5a8ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxtricks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'xtricks'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Amount 피처는 신용카드 사용 금액으로 정상/사기 트랜잭션을 결정하는 매우 중요한 속성일 가능성이 높다.\n",
    "# Amount 피처의 분포도 확인\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.xtricks(range(0, 30000, 1000), rotation=60)\n",
    "sns.distplot(card_df['Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카드 사용금액이 1000불 이하인 데이터가 대부분이며, 27000불까지 드물지만 많은 금액을 사용한 경우가 발생하면서 꼬리가 긴 형태의 분포 곡선을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount를 표준 정규 분포 형태로 변환한 뒤에 로지스틱 회귀의 예측 성능을 측정\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 사이킷런의 StandardScaler을 이용해 정규 분포 형태로 Amount 피처값 변환하는 로직으로 수정\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    amount_f = scaler.fit_trainsform(df_copy['Amount'].values.reshape(-1,1))\n",
    "    # 변환된 Amount를 Amount_Scaled로 피처명 변경 후 DataFrame 맨 앞 칼럼으로 입력\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    # 기존 Time, Amount 피처 삭제\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "lr_clf = LogisticRegression()\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('### LightGBM 예측 성능 ###')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 모델 모두 변환 이전과 비교해 성능이 크게 개선되지는 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 로그 변환 수행\n",
    "로그 변환은 데이터 분포가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중 하나.\n",
    "\n",
    "원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 변환하기 때문에 데이터 분포도의 왜곡 상당 수준을 개ㅓㄴ해준다.\n",
    "- log1p() 함수를 이용해 간단히 변환 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가공함수 get_preprocessed_df()를 로그 변환 로직으로 변경\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    # 넘파이의 log1p()를 이용해 Amount를 로그 변환\n",
    "    amoung_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('### LightGBM 예측 성능 ###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이상치 데이터 제거 후 모델 학습/예측/평가\n",
    "- 이상치 데이터(Outlier) : 전체 데이터의 패턴에서 벗어난 이상 값을 가진 데이터. 이상치로 인해 머신러닝 모델의 성능에 영향을 받는 경우가 발생하기 쉽다.\n",
    "- 이상치를 찾는 방법은 여러가지가 있지만, 이 중에서 IQR(Inter Quantile Range)방식을 적용한다.\n",
    "    - IQR은 사분위(Quantile)값의 편차를 이용하는 기법으로 흔히 박스플롯 방식으로 시각화 할 수 있다.\n",
    "        - 사분위는 전체 데이터 값이 높은 순으로 정렬하고 이를 1/4(25%)씩으로 구간을 분할하는 것을 지칭한다.\n",
    "        - 25% 구간인 Q1 ~ 75% 구간인 Q3 범위를 IQR이라고 한다.\n",
    "        \n",
    "        \n",
    "    - IQR을 이용해 이상치 데이터를 검출하는 방식 : IQR x 1.5 하여 생성된 범위를 이용해 최댓값과 최솟값을 결정한 뒤, 최댓값을 초과하거나 최솟값에 미달하는 데이터를 이상치로 간주.\n",
    "        - 3/4분위수(Q3)에 IQR x 1.5를 더해서 일반적인 데이터가 가질 수 있는 최댓값으로 가정하고, 1/4분위수(Q1)에 IQR x 1.5를 빼서 일반적인 데이터가 가질 수 있는 최솟값으로 가정\n",
    "        - 경우에 따라서 1.5가 아닌 다른 값을 적용할 수 있다. 보통은 1.5 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상치 데이터를 제거하기 전에, 어떤 피처의 이상치 데이터를 검출할 것인지 선책이 필요하다. 매우 많은 피처가 있을 경우 이들 중 결정값(즉 레이블)과 가장 상관성이 높은 피처들을 위주로 이상치를 검출하는 것이 좋다.\n",
    "(모든 피처들의 이상치를 검출하는 것은 시간이 많이 소모되며, 결정값과 상관성이 높지 않은 피처들의 경우는 이상치를 제거하더라도 크게 성능 향상에 기여하지 않기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 피처별로 상관도를 구한뒤 시각화해보기\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "corr = card_df.corr() # 피처별 상관도 구하기 \n",
    "\n",
    "# heatmap을 통한 시각화\n",
    "# cmap='RdBu'로 설정하면 양의 상관관계가 높을수록 색깔이 진한 파란색에 가까우며, 음에 상관관계가 높을 수록 색깔이 진한 빨간색에 가깝게 표현된다.\n",
    "sns.heatmap(corr, cmap='RdBu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상관관계 히트맵에서 맨 아래 위치한 결정 레이블인 class 피처와 음의 상관관계가 가장 높은 피처는 V14와 V17이다.\n",
    "\n",
    "이 중 V14에 대해서만 이상치를 찾아서 제거해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR을 이용해 이상치를 검출하는 함수 행성\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 인자로 dataframe과 이상치를 검출한 칼럼을 입력받는다. \n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    # fraud에 해당하는 column데이터만 추출, 1/4분위와 3/4분위 지점을 np.percentile로 구함.\n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    # IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최솟값 지점 구함\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    # 최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index 반환\n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_outlier 함수를 이용해 V14칼럼에서 이상치 데이터 찾기\n",
    "\n",
    "outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n",
    "print('이상치 데이터 인덱스: ', outlier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_processed_df()를 로그 변환 후 V14 피처의 이상치 데이터를 삭제하는 로직으로 변경.\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    # 이상치 데이터 삭제하는 로직 추가\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('### 로지스틱 회귀 예측 성능 ###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('\\n### LightGBM 예측 성능 ###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상치 데이터를 제거한 뒤, 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가\n",
    "- SMOTE를 적용할 떄는 반드시 학습 데이터 세트만 오버 샘플링을 해야한다.\n",
    "    - 검증 데이터 세트나 테스트 데이터 세트를 오버 샘플링할 경우, 결국은 원본 데이터 세트가 아닌 데이터 세트에서 검증 또는 테스트를 수행하기 떄문에 올바른 검증/테스트가 될 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞 예제에서 생성한 학습 피처/레이블 데이터를 SMOTE 객체의 fit_sample() 메서드를 이용해 증식 한 뒤 데이터를 증식 전과 비교f\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over, y_train_over = smote.fit_sample(X_train, y_train)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shpae, y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포:\\n', pd.Series(y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 세트는 SMOTE 적용 후 2배에 가까운 398,040건으로 데이터가 증식 되었다. \n",
    "\n",
    "그리고 레이블 값이 0과 1의 분포가 동일하게 199,020건으로 생성되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE를 통해 생성된 학습 데이터 세트를 기반으로 로지스틱 회귀 모델 학습 한 뒤 성능 평가하기\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "# ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의\n",
    "get_model_train_eval(lr_clf, ftr_trian=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀 모델의 경우 SMOTE로 오버 샘플링된 데이터로 학습할 경우 재현율이 92.47%로 크게 증가하지만, 반대로 정밀도가 5.4%로 급격하게 저하된다. 재현율이 높더라도 이 정도로 저조한 정밀도로는 현실 업무에 적용할 수 없다.\n",
    "\n",
    "이는 로지스틱 회귀 모델이 오버 샘플링으로 인해 실제 원본 데이터의 유형보다 너무나 많은 class=1 데이터를 학습하면서 실제 테스트 데이터 세트에서 예측을 지나치게 class=1로 적용해 정밀도가 급격히 떨어지게 된 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 결정 임곗값에 따른 정밀도와 재현율 곡선을 통해해 SMOTE로 학습된 로지스틱 회귀모델에 어떠한 문제 발생하고 있는지 시각화\n",
    "# 3장에서 사용한 precision_recall_curve_plot() 함수 사용\n",
    "\n",
    "def precision_recall_curve_plot(y_test, pred_proba_c1):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n",
    "    \n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임계값이 0.99이하에서는 재현율이 매우 좋고 정밀도가 극단적으로 낮다가, 0.99이상에서는 반대로 재현율이 대폭 떨어지고 정밀도가 높아진다.\n",
    "\n",
    "분류 결정 임계값을 조정하더라도 임계값의 민감도가 너무 심해 올바른 재현율/정밀도 성능을 얻을 수 없으므로 로지스틱 회귀 모델의 경우 SMOTE 적용 후 올바른 예측 모델이 생성되지 못했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LighrGBM 모델을 SMOTE로 오버샘플링된 데이터 세트로 학습/예측/평가 수행\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_trian=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재현율이 이상치만 제거한 경우인 82.88%보다 높은 84.93%이 되었지만 정밀도는 이전보다 낮은 93.23%이다.\n",
    "\n",
    "SMOTE를 적용하면 재현율은 높아지나, 정밀도는 낮아지는 것이 일반적이다. 좋은 SMOTE 패키지일수록 재현율 증가율은 높이고 정밀도 감소율은 낮출 수 있도록 효과적으로 데이터를 증식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
